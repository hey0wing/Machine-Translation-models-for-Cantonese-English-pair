{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FYP-Tensorflow",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hey0wing/Machine-Translation-models-for-Cantonese-English-pair/blob/main/FYP_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYYDvoskkE61"
      },
      "source": [
        "import glob\n",
        "!pip install --upgrade pycantonese\n",
        "import pycantonese as pc\n",
        "import re\n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNndjCpnE4Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd780b9b-e1fe-4570-a358-a7408f38e3e7"
      },
      "source": [
        "# link to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eJSTTYnkJQd"
      },
      "source": [
        "# Defineing variables\n",
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "train_size_ratio = 0.8"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wnIyQwwNw1H",
        "outputId": "5bf66e08-3dc9-4441-ee1a-9f429479142e"
      },
      "source": [
        "# read corpus data\r\n",
        "corpus = []\r\n",
        "list_of_files = glob.glob('/content/drive/My Drive/FYP corpus/*_double.txt')\r\n",
        "for file_name in list_of_files:\r\n",
        "  f = open(file_name, 'r')\r\n",
        "  corpus.extend([x for x in f.read().strip().splitlines() if x])\r\n",
        "  f.close()\r\n",
        "corpus_size = int(len(corpus)/2)\r\n",
        "print(\"Corpus size: \" + str(corpus_size))\r\n",
        "training_size = int(corpus_size*train_size_ratio)\r\n",
        "\r\n",
        "# separate Cantonese and English data\r\n",
        "corpus_yue = corpus[::2]\r\n",
        "corpus_eng = corpus[1::2]\r\n",
        "print(corpus_yue[0:9])\r\n",
        "print(corpus_eng[0:9])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus size: 322\n",
            "['打還打嗎', '係要摷亂個頭', '咁梗係啦', '唔係幫手做咩呀', '咁就可以講個笑話啦', '我驚你今日冇咁順利', '你驚我返嚟搞破壞？', '唔會啩咁秘密', '我唔係講你daddy呀']\n",
            "['Fighting is okay.', \"But don't mess up my hair.\", 'Of course.', \"Or else, I wouldn't ask for your help.\", 'Then, can I be honest?', 'I have a bad feeling about this.', 'Will Dad stop the wedding?', 'How will he find out?', 'Forget about your dad.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NljUUYpWVwX-"
      },
      "source": [
        "# Converts the unicode file to ascii\r\n",
        "def unicode_to_ascii(s):\r\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\r\n",
        "      if unicodedata.category(c) != 'Mn')\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_sentence(w):\r\n",
        "  w = unicode_to_ascii(w.lower().strip())\r\n",
        "\r\n",
        "  # creating a space between a word and the punctuation following it\r\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\r\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\r\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\r\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\r\n",
        "\r\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\r\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\r\n",
        "\r\n",
        "  w = w.strip()\r\n",
        "  return w"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRMzIiXjEnKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e51829d-00fe-4c7f-ce9c-6aa1911fc238"
      },
      "source": [
        "# segmentation of Cantonese\r\n",
        "preprocess_yue = []\r\n",
        "for sentence_yue in corpus_yue:\r\n",
        "  preprocess_yue.append(pc.segment(sentence_yue))\r\n",
        "print(preprocess_yue[0:10])\r\n",
        "\r\n",
        "# segmentation of English\r\n",
        "preprocess_eng = []\r\n",
        "for sentence_eng in corpus_eng:\r\n",
        "  preprocess_eng.append(preprocess_sentence(sentence_eng))\r\n",
        "print(preprocess_eng[0:10])  "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['打', '還', '打', '嗎'], ['係要', '摷', '亂', '個頭'], ['咁', '梗係啦'], ['唔係', '幫手', '做咩', '呀'], ['咁', '就可以', '講', '個', '笑話', '啦'], ['我', '驚', '你', '今日', '冇', '咁', '順利'], ['你', '驚', '我', '返嚟', '搞', '破壞', '？'], ['唔會啩', '咁', '秘密'], ['我', '唔係', '講', '你', 'daddy', '呀'], ['我', '講', '新郎哥', '個', '小姐', '放心', '啦']]\n",
            "['fighting is okay .', 'but don t mess up my hair .', 'of course .', 'or else , i wouldn t ask for your help .', 'then , can i be honest ?', 'i have a bad feeling about this .', 'will dad stop the wedding ?', 'how will he find out ?', 'forget about your dad .', 'where s your groom ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1sD-7v0kYWk"
      },
      "source": [
        "# Separating training and testing set\n",
        "training_sentences_yue = preprocess_yue[0:training_size]\n",
        "testing_sentences_yue = preprocess_yue[training_size:]\n",
        "\n",
        "training_sentences_eng = preprocess_eng[0:training_size]\n",
        "testing_sentences_eng = preprocess_eng[training_size:]"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u8UB0MCkZ5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0deecd00-060f-4404-ed3e-c07f5b024b68"
      },
      "source": [
        "# tokenizing and padding for Cantonese\n",
        "tokenizer_yue = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer_yue.fit_on_texts(training_sentences_yue)\n",
        "word_index_yue = tokenizer_yue.word_index\n",
        "\n",
        "training_sequences_yue = tokenizer_yue.texts_to_sequences(training_sentences_yue)\n",
        "testing_sequences_yue = tokenizer_yue.texts_to_sequences(testing_sentences_yue)\n",
        "max_length_yue = len(max(training_sequences_yue, key=len))\n",
        "\n",
        "training_padded_yue = pad_sequences(training_sequences_yue, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "testing_padded_yue = pad_sequences(testing_sequences_yue, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Padding size for Cantonese: \" + str(max_length_yue))\n",
        "print(training_padded_yue)\n",
        "\n",
        "# tokenizing and padding for Cantonese\n",
        "tokenizer_eng = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer_eng.fit_on_texts(training_sentences_eng)\n",
        "word_index_eng = tokenizer_eng.word_index\n",
        "\n",
        "training_sequences_eng = tokenizer_eng.texts_to_sequences(training_sentences_eng)\n",
        "testing_sequences_eng = tokenizer_eng.texts_to_sequences(testing_sentences_eng)\n",
        "max_length_eng = len(max(training_sequences_eng, key=len))\n",
        "\n",
        "training_padded_eng = pad_sequences(training_sequences_eng, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "testing_padded_eng = pad_sequences(testing_sequences_eng, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(\"Padding size for English: \" + str(max_length_eng))\n",
        "print(training_padded_eng)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padding size for Cantonese: 27\n",
            "[[ 47  79  47 ...   0   0   0]\n",
            " [229 230 231 ...   0   0   0]\n",
            " [  7 118   0 ...   0   0   0]\n",
            " ...\n",
            " [ 35 224  26 ...   0   0   0]\n",
            " [  2 792   2 ...   0   0   0]\n",
            " [796   8   3 ...   0   0   0]]\n",
            "Padding size for English: 24\n",
            "[[107  13 242 ...   0   0   0]\n",
            " [ 28  38   5 ...   0   0   0]\n",
            " [ 24 153   0 ...   0   0   0]\n",
            " ...\n",
            " [ 50   6 144 ...   0   0   0]\n",
            " [ 46  39   2 ...   0   0   0]\n",
            " [ 10 165   5 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrAlWBKf99Ya"
      },
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FufaT4vlkiDE"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfDt1hmYkiys"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DTKQFf1kkyc"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HYfBKXjkmU8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SBdAZAenvzL"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_sentence(training_padded[0]))\n",
        "print(training_sentences[2])\n",
        "print(labels[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9MqihtEkzQ9"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoBXVffknldU"
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4eZ5HtVnnEE"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG8-ArY-qDcz"
      },
      "source": [
        "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}