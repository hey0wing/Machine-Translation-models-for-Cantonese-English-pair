{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u7y4if32xY-g"
   },
   "outputs": [],
   "source": [
    "# Statistical Machine Translation System\n",
    "# English to Cantonese\n",
    "\n",
    "# IBM Model 1 for Word Translation Task\n",
    "# Word Alignment based on Relative Positions\n",
    "# Bi-gram Language Modelling with Laplace Smoothing and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MsJzVKnIcb",
    "outputId": "74abecec-ed46-49f3-fee7-caf69e7d48ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hey0\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2cVVv4e0VoF0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aKVfJuwSxY-w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...</td>\n",
       "      <td>I was not allowed to touch or eat pickles I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>佢地扮怪面嚇人</td>\n",
       "      <td>They were making scary faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>唔會搞到我哋變得邪惡女人型變成自由黨人咩</td>\n",
       "      <td>turn us into godless sissy liberals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呢個模式可以清晰令您瞭解佢哋</td>\n",
       "      <td>So it spells those out in very clean terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>幾多萬億掌聲</td>\n",
       "      <td>How many trillions Applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0  唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...   \n",
       "1                                            佢地扮怪面嚇人   \n",
       "2                               唔會搞到我哋變得邪惡女人型變成自由黨人咩   \n",
       "3                                     呢個模式可以清晰令您瞭解佢哋   \n",
       "4                                             幾多萬億掌聲   \n",
       "\n",
       "                                                 eng  \n",
       "0  I was not allowed to touch or eat pickles I wa...  \n",
       "1                       They were making scary faces  \n",
       "2                turn us into godless sissy liberals  \n",
       "3         So it spells those out in very clean terms  \n",
       "4                        How many trillions Applause  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "all_csv = glob.glob(os.getcwd() + \"/train.csv\")  \n",
    "\n",
    "df_test = pd.read_csv(os.getcwd() + \"/test.csv\", sep='\\t', encoding='utf-8')  \n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, sep='\\t', encoding='utf-8') for f in all_csv)\n",
    "df = pd.concat(df_from_each_file)\n",
    "\n",
    "# Check for null\n",
    "df[df['yue'].isnull()]\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "YueChar = True\n",
    "# Delete spaces between n-gram in Cantonese\n",
    "# Perform Character based tokenization in Cantonese\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].str.replace(r' ', '')\n",
    "    df_test['yue'] = df_test['yue'].str.replace(r' ', '')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我 相 信 主</td>\n",
       "      <td>I believe the almighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友</td>\n",
       "      <td>For too long now children with developmental d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一 般 會 遇 到 兩 種 反 應</td>\n",
       "      <td>I have two kinds of reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...</td>\n",
       "      <td>But think about this The IMF official is hardl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>佢 將 呢 個 病 毒 傳 畀 BB</td>\n",
       "      <td>She passes that virus on to baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0                                            我 相 信 主   \n",
       "1                          好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友   \n",
       "2                                  一 般 會 遇 到 兩 種 反 應   \n",
       "3  但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...   \n",
       "4                                 佢 將 呢 個 病 毒 傳 畀 BB   \n",
       "\n",
       "                                                 eng  \n",
       "0                             I believe the almighty  \n",
       "1  For too long now children with developmental d...  \n",
       "2                      I have two kinds of reactions  \n",
       "3  But think about this The IMF official is hardl...  \n",
       "4                   She passes that virus on to baby  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "def spliteKeyWord(str):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "    df_test['yue'] = df_test['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['They', 'were', 'making', 'scary', 'faces'],\n",
       "  ['turn', 'us', 'into', 'godless', 'sissy', 'liberals'],\n",
       "  ['So', 'it', 'spells', 'those', 'out', 'in', 'very', 'clean', 'terms']],\n",
       " [['佢', '地', '扮', '怪', '面', '嚇', '人'],\n",
       "  ['唔',\n",
       "   '會',\n",
       "   '搞',\n",
       "   '到',\n",
       "   '我',\n",
       "   '哋',\n",
       "   '變',\n",
       "   '得',\n",
       "   '邪',\n",
       "   '惡',\n",
       "   '女',\n",
       "   '人',\n",
       "   '型',\n",
       "   '變',\n",
       "   '成',\n",
       "   '自',\n",
       "   '由',\n",
       "   '黨',\n",
       "   '人',\n",
       "   '咩'],\n",
       "  ['呢', '個', '模', '式', '可', '以', '清', '晰', '令', '您', '瞭', '解', '佢', '哋']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "inp_lang = 'eng'\n",
    "tar_lang = 'yue'\n",
    "\n",
    "def df_filter(df):\n",
    "    return df[\n",
    "        df['yue'].apply(lambda x: len(x.split())<MAX_LENGTH) &\n",
    "        df['eng'].apply(lambda x: len(x.split())<MAX_LENGTH) \n",
    "    ]\n",
    "df = df_filter(df)\n",
    "df_test = df_filter(df_test)\n",
    "\n",
    "inp = [i.split() for i in df[inp_lang].to_list()]\n",
    "tar = [i.split() for i in df[tar_lang].to_list()]\n",
    "inp_test = [i.split() for i in df_test[inp_lang].to_list()]\n",
    "tar_test = [i.split() for i in df_test[tar_lang].to_list()]\n",
    "inp[:3], tar[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIZNBPNAxY_S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 1696)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(inp)\n",
    "test_size = len(inp_test)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VMMiv4BxY_c",
    "outputId": "589049e5-2913-4453-d45f-0465ddcd9683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:\n",
      "eng : 6219\n",
      "yue : 2705\n"
     ]
    }
   ],
   "source": [
    "inp_words = {}\n",
    "tar_words = {}\n",
    "\n",
    "for sentence in inp:\n",
    "    for word in sentence:\n",
    "        if word in inp_words:\n",
    "            inp_words[word] += 1\n",
    "        else:\n",
    "            inp_words[word] = 1\n",
    "            \n",
    "for sentence in tar:\n",
    "    for word in sentence:\n",
    "        if word in tar_words:\n",
    "            tar_words[word] += 1\n",
    "        else:\n",
    "            tar_words[word] = 1\n",
    "                    \n",
    "inp_vocab = len(inp_words)\n",
    "tar_vocab = len(tar_words)\n",
    "print(\"Number of Unique Words:\")\n",
    "print(inp_lang, ':', str(inp_vocab))\n",
    "print(tar_lang, ':', str(tar_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCA1636BxY_k"
   },
   "outputs": [],
   "source": [
    "# creating the 't'\n",
    "t = {}\n",
    "# usage: t[('inp_word', 'tar_word')] = probability of inp_Word given tar_word\n",
    "uniform = 1 / (inp_vocab * tar_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 128\n",
    "word_factor_max = 2\n",
    "\n",
    "fine_tune = 1\n",
    "has_converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2021-04-16 14:52:16.435526: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
       "2021-04-16 14:52:16.435695: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
       "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
       "                   [--path_prefix PATH] [--window_title TEXT]\n",
       "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
       "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: invalid choice: 'sem1COMP4801JupyterlogSMT-2_EngYue' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id = f\"SMT-2_EngYue\"\n",
    "log_dir = os.path.join(os.path.join(os.getcwd(), 'log'), run_id)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKUGRo00xY_p",
    "outputId": "ad2a8a33-2b25-4340-9525-a18ab4519552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Completed, Maximum Change: 0.8167420220033955\n",
      "Time: 20.438411474227905 secs\n",
      "Iteration 2 Completed, Maximum Change: 0.3456606319708041\n",
      "Time: 17.682696104049683 secs\n",
      "Iteration 3 Completed, Maximum Change: 0.18087302712841014\n",
      "Time: 17.102671146392822 secs\n",
      "Iteration 4 Completed, Maximum Change: 0.10117600644019553\n",
      "Time: 20.732101678848267 secs\n",
      "Iteration 5 Completed, Maximum Change: 0.059758809816697234\n",
      "Time: 19.197176456451416 secs\n",
      "Iteration 6 Completed, Maximum Change: 0.03809698662111716\n",
      "Time: 21.44188356399536 secs\n",
      "Iteration 7 Completed, Maximum Change: 0.027846464415475086\n",
      "Time: 20.426174879074097 secs\n",
      "Iteration 8 Completed, Maximum Change: 0.023560604238536953\n",
      "Time: 18.342814207077026 secs\n",
      "Iteration 9 Completed, Maximum Change: 0.019136175287770896\n",
      "Time: 20.06472897529602 secs\n",
      "Iteration 10 Completed, Maximum Change: 0.015195646884349101\n",
      "Time: 19.027939319610596 secs\n",
      "Iteration 11 Completed, Maximum Change: 0.012661626124247427\n",
      "Time: 19.509644269943237 secs\n",
      "Iteration 12 Completed, Maximum Change: 0.011946971919487126\n",
      "Time: 16.908743381500244 secs\n",
      "Iteration 13 Completed, Maximum Change: 0.011320710375484322\n",
      "Time: 15.892218351364136 secs\n",
      "Iteration 14 Completed, Maximum Change: 0.01072667056619081\n",
      "Time: 15.987308979034424 secs\n",
      "Iteration 15 Completed, Maximum Change: 0.010167581631155831\n",
      "Time: 19.10481095314026 secs\n",
      "Iteration 16 Completed, Maximum Change: 0.00964486685050589\n",
      "Time: 15.812053442001343 secs\n",
      "Iteration 17 Completed, Maximum Change: 0.009171150942424877\n",
      "Time: 15.5124831199646 secs\n",
      "Iteration 18 Completed, Maximum Change: 0.008730189407005762\n",
      "Time: 12.842923879623413 secs\n",
      "Iteration 19 Completed, Maximum Change: 0.008316897995895633\n",
      "Time: 16.787328481674194 secs\n",
      "Iteration 20 Completed, Maximum Change: 0.007928454925579698\n",
      "Time: 13.277393102645874 secs\n",
      "Iteration 21 Completed, Maximum Change: 0.007562452636883821\n",
      "Time: 13.468306064605713 secs\n",
      "Iteration 22 Completed, Maximum Change: 0.0072168099194956925\n",
      "Time: 16.97100853919983 secs\n",
      "Iteration 23 Completed, Maximum Change: 0.006889744492109551\n",
      "Time: 19.889899253845215 secs\n",
      "Iteration 24 Completed, Maximum Change: 0.006579749215849473\n",
      "Time: 24.019300937652588 secs\n",
      "Iteration 25 Completed, Maximum Change: 0.006285549653817335\n",
      "Time: 17.6406888961792 secs\n",
      "Iteration 26 Completed, Maximum Change: 0.006006051385339273\n",
      "Time: 19.446605682373047 secs\n",
      "Iteration 27 Completed, Maximum Change: 0.0057402932258538986\n",
      "Time: 24.48550009727478 secs\n",
      "Iteration 28 Completed, Maximum Change: 0.005487414373935184\n",
      "Time: 23.913451433181763 secs\n",
      "Iteration 29 Completed, Maximum Change: 0.005246634231866332\n",
      "Time: 21.983316898345947 secs\n",
      "Iteration 30 Completed, Maximum Change: 0.005017240105827159\n",
      "Time: 21.771398067474365 secs\n",
      "Iteration 31 Completed, Maximum Change: -1\n",
      "Time: 20.894391536712646 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_iters = 0\n",
    "\n",
    "while n_iters < max_iters and has_converged == False:\n",
    "    start = time.time()\n",
    "    has_converged = True\n",
    "    max_change = -1\n",
    "\n",
    "    n_iters += 1\n",
    "    count = {}\n",
    "    total = {}\n",
    "    for index in range(train_size):\n",
    "        s_total = {}\n",
    "        for inp_word in inp[index]:\n",
    "            s_total[inp_word] = 0\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in t:\n",
    "                    t[(inp_word, tar_word)] = uniform\n",
    "                s_total[inp_word] += t[(inp_word, tar_word)]\n",
    "\n",
    "        for inp_word in inp[index]:\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in count:\n",
    "                    count[(inp_word, tar_word)] = 0\n",
    "                count[(inp_word, tar_word)] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "                if tar_word not in total:\n",
    "                    total[tar_word] = 0\n",
    "                total[tar_word] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "    # estimating the probabilities\n",
    "\n",
    "    if fine_tune == 0:\n",
    "      updated = {}\n",
    "      # train for all valid word pairs s.t count(inp_word, tar_word) > 0\n",
    "      for index in range(train_size):\n",
    "          for tar_word in tar[index]:\n",
    "              for inp_word in inp[index]:\n",
    "                  if (inp_word, tar_word) in updated:\n",
    "                      continue\n",
    "                  updated[(inp_word, tar_word)] = 1\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.01:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "\n",
    "    elif fine_tune == 1:\n",
    "      # train it only for 1000 most frequent words in English and Cantonese\n",
    "      n_tar_words = 0\n",
    "      updates = 0\n",
    "\n",
    "      for tar_word_tuples in sorted(tar_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "          tar_word = tar_word_tuples[0]\n",
    "          n_tar_words += 1\n",
    "          if n_tar_words > len(tar_words)/word_factor_max:\n",
    "              break\n",
    "          n_inp_words = 0\n",
    "          for inp_word_tuples in sorted(inp_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "              inp_word = inp_word_tuples[0]\n",
    "              n_inp_words += 1\n",
    "              if n_inp_words > len(inp_words)/word_factor_max:\n",
    "                  break\n",
    "              if (inp_word, tar_word) not in count or tar_word not in total:\n",
    "                  continue\n",
    "                  # assume in this case: t[(inp_word, tar_word)] = uniform\n",
    "              else:\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.005:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "                \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Change\", max_change, step=n_iters)\n",
    "\n",
    "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change) + '\\nTime: {} secs'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yaalpW64cA-",
    "outputId": "b0678767-12f0-42ed-ab2f-c4fd45fe9a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Applause', '掌'), 0.993822137807804)\n",
      "(('Guitar', '吉'), 0.9602192290684652)\n",
      "(('Laughter', '笑'), 0.9443585196973706)\n",
      "(('or', '或'), 0.8445960533949959)\n",
      "(('play', '玩'), 0.7203063887893961)\n",
      "(('cancer', '癌'), 0.719850020241164)\n",
      "(('I', '我'), 0.669443309056773)\n",
      "(('because', '因'), 0.6601850305401723)\n",
      "(('you', '你'), 0.6550438780164455)\n",
      "(('and', '和'), 0.6464206172605996)\n",
      "(('Thank', '謝'), 0.6028772693167712)\n",
      "(('information', '息'), 0.5999382245120436)\n",
      "(('serious', '嚴'), 0.5943731086801906)\n",
      "(('like', '似'), 0.5868679633177576)\n",
      "(('carousel', '迴'), 0.5848280179345284)\n",
      "(('know', '知'), 0.5707580790486463)\n",
      "(('and', '及'), 0.5662515494161114)\n",
      "(('two', '兩'), 0.5602898592921483)\n",
      "(('apartment', '寓'), 0.5521367285349217)\n",
      "(('Bible', '聖'), 0.5420702303674806)\n",
      "(('Laughter', '聲'), 0.53644230213268)\n",
      "(('So', '所'), 0.5320624787227191)\n",
      "(('fell', '跌'), 0.5301091595256303)\n",
      "(('and', '同'), 0.5298744938454806)\n",
      "(('love', '愛'), 0.5280797775404907)\n",
      "(('revolution', '革'), 0.5099479899415553)\n",
      "(('Applause', '鼓'), 0.504960032204762)\n",
      "(('more', '更'), 0.500858882647874)\n",
      "(('brain', '腦'), 0.4978312085432045)\n",
      "(('the', '界'), 0.48909354573596764)\n",
      "(('we', '哋'), 0.4877140196604776)\n",
      "(('change', '改'), 0.48746949006148327)\n",
      "(('the', '面'), 0.48368410483397833)\n",
      "(('heart', '臟'), 0.48250684626374973)\n",
      "(('to', '想'), 0.48177401632858785)\n",
      "(('Paris', '黎'), 0.48134213043245283)\n",
      "(('autism', '閉'), 0.4745769538002506)\n",
      "(('and', '埋'), 0.4722544156030727)\n",
      "(('to', '去'), 0.47014871997231344)\n",
      "(('pain', '痛'), 0.46987465448459476)\n"
     ]
    }
   ],
   "source": [
    "# displaying the most confident translation pairs\n",
    "limit = 40\n",
    "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "  print(element)\n",
    "  limit -= 1\n",
    "  if limit <= 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DpMllrHFDYV-"
   },
   "outputs": [],
   "source": [
    "# saving the translation model\n",
    "file = open(\"translation_model.pkl\",\"wb\")\n",
    "pickle.dump(t, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4wQ0a-EARrIy"
   },
   "outputs": [],
   "source": [
    "# using the model trained until convergence\n",
    "# to use a saved model\n",
    "model_name = \"translation_model.pkl\"\n",
    "pickle_in = open(model_name,\"rb\")\n",
    "t = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yaz1CgJoCllZ"
   },
   "outputs": [],
   "source": [
    "I = {}\n",
    "for index in range(train_size):\n",
    "    for inp_id in range(len(inp[index])):\n",
    "        length = len(inp[index])\n",
    "        if length not in I:\n",
    "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
    "        for tar_id in range(len(tar[index])):\n",
    "            if (tar_id - inp_id) not in I[length]:\n",
    "                I[length][(tar_id - inp_id)] = [t[(inp[index][inp_id], tar[index][tar_id])], 1]\n",
    "            else:\n",
    "                I[length][(tar_id - inp_id)][0] += t[(inp[index][inp_id], tar[index][tar_id])]\n",
    "                I[length][(tar_id - inp_id)][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh6ogFdO2r_-",
    "outputId": "0d071c8f-b63d-407b-9d3a-c13e8308f2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "# viewing the available sentence lengths encountered during training\n",
    "sentence_lengths = []\n",
    "for key in I.keys():\n",
    "    if key not in sentence_lengths:\n",
    "        sentence_lengths.append(key)\n",
    "sentence_lengths.sort()\n",
    "print(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1CNo6HGz6lqK"
   },
   "outputs": [],
   "source": [
    "# computing the alignment probabilities\n",
    "# p[I][tar_id - inp_id] = p(i | i', I)\n",
    "\n",
    "p = {}\n",
    "for key in I.keys():\n",
    "    p[key] = {}\n",
    "    sum_val = 0\n",
    "    for diff in I[key].keys():\n",
    "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
    "        sum_val += p[key][diff]\n",
    "    for diff in p[key].keys():\n",
    "        p[key][diff] /= sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWI7HCoI81Qa",
    "outputId": "543de11e-d4e9-4b17-d13f-262a28b78541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 14\n",
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 13\n"
     ]
    }
   ],
   "source": [
    "for index in range(train_size):\n",
    "    length_inp = len(inp[index])\n",
    "    length_tar = len(tar[index])\n",
    "    if length_tar - length_inp > 10 and length_inp == 1:\n",
    "        print(\"Length of English Sentence:\", str(length_inp))\n",
    "        print(\"Length of Cantonese Sentence:\", str(length_tar))\n",
    "        \n",
    "# there exists an English sentence with one token s.t the Cantonese translation contains 19 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yqzyU-2g9_jk"
   },
   "outputs": [],
   "source": [
    "# computing initial transitions\n",
    "init = {}\n",
    "for length in p:\n",
    "    max_prob = -1\n",
    "    max_jump = 0\n",
    "    for key in p[length].keys():\n",
    "        if p[length][key] > max_prob:\n",
    "            max_prob = p[length][key]\n",
    "            max_jump = key\n",
    "    init[length] = max_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlM2-MzmExbc",
    "outputId": "c3791846-4116-4905-88db-6d78866b792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Bigrams: 27343\n",
      "Number of Unique Unigrams: 2705\n"
     ]
    }
   ],
   "source": [
    "# computing the transition probabilities for Cantonese\n",
    "bigrams = {}\n",
    "unigrams = {}\n",
    "\n",
    "# training on the train_set\n",
    "def model(dataset_size, dataset_name):\n",
    "    global bigrams\n",
    "    global unigrams\n",
    "    for index in range(dataset_size):\n",
    "        token_A = ''\n",
    "        for tar_token in tar[index]:\n",
    "            if tar_token not in unigrams:\n",
    "                unigrams[tar_token] = 1\n",
    "            else:\n",
    "                unigrams[tar_token] += 1\n",
    "            \n",
    "            token_B = tar_token\n",
    "            if (token_A, token_B) not in bigrams:\n",
    "                bigrams[(token_A, token_B)] = 1\n",
    "            else:\n",
    "                bigrams[(token_A, token_B)] += 1\n",
    "            token_A = token_B\n",
    "\n",
    "model(train_size, 'tar_train')\n",
    "\n",
    "bigram_count = len(bigrams)\n",
    "unigram_count = len(unigrams)\n",
    "print(\"Number of Unique Bigrams:\", bigram_count)\n",
    "print(\"Number of Unique Unigrams:\", unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AF98ePawMnaE",
    "outputId": "5f2fd7f7-3702-4fb0-807a-ee332d5ab102"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "computed_sentences = []\n",
    "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
    "null_BLEU_count = 0\n",
    "\n",
    "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
    "\n",
    "def find_translation(inp_token):\n",
    "    for element in sorted_t:\n",
    "        if element[0][0].lower() == inp_token:\n",
    "            return element[0][1]\n",
    "    return \"\"\n",
    "\n",
    "def get_prob(seq):\n",
    "    # bigram language model with laplace smoothing and backoff\n",
    "    if len(seq) < 2 or len(seq) > 10:\n",
    "        return 1\n",
    "    score = 0\n",
    "    token_A = ''\n",
    "    for tar_token in seq:\n",
    "        token_B = tar_token\n",
    "        if (token_A, token_B) not in bigrams:\n",
    "            if token_B not in unigrams:\n",
    "                continue\n",
    "            else:\n",
    "                score += unigrams[token_B] / unigram_count\n",
    "        else:\n",
    "            base_token_count = 0\n",
    "            if token_A in unigrams:\n",
    "                base_token_count = unigrams[token_A]\n",
    "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
    "        token_A = token_B\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 / 1696 in 18.91235852241516 s\n",
      "Progress: 100 / 1696 in 15.70876145362854 s\n",
      "Progress: 150 / 1696 in 23.508580923080444 s\n",
      "Progress: 200 / 1696 in 15.550612211227417 s\n",
      "Progress: 250 / 1696 in 10.111546754837036 s\n",
      "Progress: 300 / 1696 in 11.902549266815186 s\n",
      "Progress: 350 / 1696 in 16.584648609161377 s\n",
      "Progress: 400 / 1696 in 10.494550704956055 s\n",
      "Progress: 450 / 1696 in 13.929544687271118 s\n",
      "Progress: 500 / 1696 in 14.463541269302368 s\n",
      "Progress: 550 / 1696 in 11.39703369140625 s\n",
      "Progress: 600 / 1696 in 12.185068130493164 s\n",
      "Progress: 650 / 1696 in 15.048619508743286 s\n",
      "Progress: 700 / 1696 in 13.103516578674316 s\n",
      "Progress: 750 / 1696 in 14.15402889251709 s\n",
      "Progress: 800 / 1696 in 20.21412706375122 s\n",
      "Progress: 850 / 1696 in 16.793059587478638 s\n",
      "Progress: 900 / 1696 in 14.0885169506073 s\n",
      "Progress: 950 / 1696 in 16.252156496047974 s\n",
      "Progress: 1000 / 1696 in 15.219541549682617 s\n",
      "Progress: 1050 / 1696 in 14.501519680023193 s\n",
      "Progress: 1100 / 1696 in 8.862546682357788 s\n",
      "Progress: 1150 / 1696 in 14.464101314544678 s\n",
      "Progress: 1200 / 1696 in 13.196029901504517 s\n",
      "Progress: 1250 / 1696 in 18.291574716567993 s\n",
      "Progress: 1300 / 1696 in 16.98509192466736 s\n",
      "Progress: 1350 / 1696 in 15.286030292510986 s\n",
      "Progress: 1400 / 1696 in 12.39702820777893 s\n",
      "Progress: 1450 / 1696 in 9.870026588439941 s\n",
      "Progress: 1500 / 1696 in 13.05600118637085 s\n",
      "Progress: 1550 / 1696 in 14.200575828552246 s\n",
      "Progress: 1600 / 1696 in 15.333029508590698 s\n",
      "Progress: 1650 / 1696 in 21.22907829284668 s\n",
      "\n",
      "ToTal: 1696 / 1696 in 497.72607040405273 s\n"
     ]
    }
   ],
   "source": [
    "TotalTime = time.time()\n",
    "start = time.time()\n",
    "\n",
    "ori = []\n",
    "ref = []\n",
    "can = []\n",
    "num_test = 0\n",
    "\n",
    "for index in range(test_size):\n",
    "\n",
    "    translated_words = []\n",
    "    for inp_token in inp_test[index]:\n",
    "        translation = find_translation(inp_token)\n",
    "        if translation != \"\":\n",
    "            translated_words.append(translation)\n",
    "    perm = permutations(translated_words)\n",
    "    best_seq = translated_words\n",
    "    best_prob = -1\n",
    "\n",
    "    if len(inp_test[index]) < 10 and len(inp_test[index]) > 2:\n",
    "        for seq in perm:            \n",
    "            prob = get_prob(seq)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_seq = seq\n",
    "            \n",
    "    ori.append(inp_test[index])\n",
    "    ref.append(tar_test[index])\n",
    "    can.append(best_seq)\n",
    "    num_test += 1\n",
    "    \n",
    "    if num_test % 50 == 0:\n",
    "        print(f'Progress: {num_test} / {test_size} in {time.time()-start} s')\n",
    "        start = time.time()\n",
    "        \n",
    "print(f'\\nToTal: {num_test} / {test_size} in {time.time()-TotalTime} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['because', 'when', 'the', 'self', 'is', 'suspended']\n",
      "['因', '為', '當', '我', '的', '自', '我', '被', '暫', '停', '後']\n",
      "('界', '因', '當', '自', '是', '暫')\n",
      "BLEU: 103.96602697150126\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import random\n",
    "ran = random.randint(1, len(ref))\n",
    "print(ori[ran])\n",
    "print(ref[ran])\n",
    "print(can[ran])\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "print('BLEU:', sentence_bleu(ref[ran], can[ran], smoothing_function=smoothie)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcR-JpzNT8gp",
    "outputId": "332fdf47-06f4-4e1b-8772-ba6b0357491a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-c: 0.18080344988234295\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based and average score\n",
    "score = 0\n",
    "# for i in range(len(ref)):\n",
    "#     r = ref[i]\n",
    "#     c = can[i]\n",
    "#     score += sentence_bleu(r, c, smoothing_function=smoothie)*100\n",
    "# print('BLEU-s:', score/len(ref))\n",
    "\n",
    "# Corpus based, summing all nominator and denominator before division\n",
    "# r = [[r.split()] for r in ref]\n",
    "# c = [c.split() for c in can]\n",
    "score = corpus_bleu(ref, can, smoothing_function=smoothie)*100\n",
    "print('BLEU-c:', score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SMT_English_to_Hindi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
