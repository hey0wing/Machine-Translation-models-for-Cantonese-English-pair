{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u7y4if32xY-g"
   },
   "outputs": [],
   "source": [
    "# Statistical Machine Translation System\n",
    "# English to Cantonese\n",
    "\n",
    "# IBM Model 1 for Word Translation Task\n",
    "# Word Alignment based on Relative Positions\n",
    "# Bi-gram Language Modelling with Laplace Smoothing and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MsJzVKnIcb",
    "outputId": "74abecec-ed46-49f3-fee7-caf69e7d48ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hey0\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: click in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2cVVv4e0VoF0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aKVfJuwSxY-w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...</td>\n",
       "      <td>I was not allowed to touch or eat pickles I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>佢地扮怪面嚇人</td>\n",
       "      <td>They were making scary faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>唔會搞到我哋變得邪惡女人型變成自由黨人咩</td>\n",
       "      <td>turn us into godless sissy liberals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呢個模式可以清晰令您瞭解佢哋</td>\n",
       "      <td>So it spells those out in very clean terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>幾多萬億掌聲</td>\n",
       "      <td>How many trillions Applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0  唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...   \n",
       "1                                            佢地扮怪面嚇人   \n",
       "2                               唔會搞到我哋變得邪惡女人型變成自由黨人咩   \n",
       "3                                     呢個模式可以清晰令您瞭解佢哋   \n",
       "4                                             幾多萬億掌聲   \n",
       "\n",
       "                                                 eng  \n",
       "0  I was not allowed to touch or eat pickles I wa...  \n",
       "1                       They were making scary faces  \n",
       "2                turn us into godless sissy liberals  \n",
       "3         So it spells those out in very clean terms  \n",
       "4                        How many trillions Applause  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "all_csv = glob.glob(os.getcwd() + \"/train.csv\")  \n",
    "\n",
    "df_test = pd.read_csv(os.getcwd() + \"/test.csv\", sep='\\t', encoding='utf-8')  \n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, sep='\\t', encoding='utf-8') for f in all_csv)\n",
    "df = pd.concat(df_from_each_file)\n",
    "\n",
    "# Check for null\n",
    "df[df['yue'].isnull()]\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "YueChar = True\n",
    "# Delete spaces between n-gram in Cantonese\n",
    "# Perform Character based tokenization in Cantonese\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].str.replace(r' ', '')\n",
    "    df_test['yue'] = df_test['yue'].str.replace(r' ', '')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我 相 信 主</td>\n",
       "      <td>I believe the almighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友</td>\n",
       "      <td>For too long now children with developmental d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一 般 會 遇 到 兩 種 反 應</td>\n",
       "      <td>I have two kinds of reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...</td>\n",
       "      <td>But think about this The IMF official is hardl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>佢 將 呢 個 病 毒 傳 畀 BB</td>\n",
       "      <td>She passes that virus on to baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0                                            我 相 信 主   \n",
       "1                          好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友   \n",
       "2                                  一 般 會 遇 到 兩 種 反 應   \n",
       "3  但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...   \n",
       "4                                 佢 將 呢 個 病 毒 傳 畀 BB   \n",
       "\n",
       "                                                 eng  \n",
       "0                             I believe the almighty  \n",
       "1  For too long now children with developmental d...  \n",
       "2                      I have two kinds of reactions  \n",
       "3  But think about this The IMF official is hardl...  \n",
       "4                   She passes that virus on to baby  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "def spliteKeyWord(str):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "    df_test['yue'] = df_test['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['They', 'were', 'making', 'scary', 'faces'],\n",
       "  ['turn', 'us', 'into', 'godless', 'sissy', 'liberals'],\n",
       "  ['So', 'it', 'spells', 'those', 'out', 'in', 'very', 'clean', 'terms']],\n",
       " [['佢', '地', '扮', '怪', '面', '嚇', '人'],\n",
       "  ['唔',\n",
       "   '會',\n",
       "   '搞',\n",
       "   '到',\n",
       "   '我',\n",
       "   '哋',\n",
       "   '變',\n",
       "   '得',\n",
       "   '邪',\n",
       "   '惡',\n",
       "   '女',\n",
       "   '人',\n",
       "   '型',\n",
       "   '變',\n",
       "   '成',\n",
       "   '自',\n",
       "   '由',\n",
       "   '黨',\n",
       "   '人',\n",
       "   '咩'],\n",
       "  ['呢', '個', '模', '式', '可', '以', '清', '晰', '令', '您', '瞭', '解', '佢', '哋']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "inp_lang = 'eng'\n",
    "tar_lang = 'yue'\n",
    "\n",
    "def df_filter(df):\n",
    "    return df[\n",
    "        df['yue'].apply(lambda x: len(x.split())<MAX_LENGTH) &\n",
    "        df['eng'].apply(lambda x: len(x.split())<MAX_LENGTH) \n",
    "    ]\n",
    "df = df_filter(df)\n",
    "df_test = df_filter(df_test)\n",
    "\n",
    "inp = [i.split() for i in df[inp_lang].to_list()]\n",
    "tar = [i.split() for i in df[tar_lang].to_list()]\n",
    "inp_test = [i.split() for i in df_test[inp_lang].to_list()]\n",
    "tar_test = [i.split() for i in df_test[tar_lang].to_list()]\n",
    "inp[:3], tar[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIZNBPNAxY_S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 1696)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(inp)\n",
    "test_size = len(inp_test)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VMMiv4BxY_c",
    "outputId": "589049e5-2913-4453-d45f-0465ddcd9683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:\n",
      "eng : 6219\n",
      "yue : 2705\n"
     ]
    }
   ],
   "source": [
    "inp_words = {}\n",
    "tar_words = {}\n",
    "\n",
    "for sentence in inp:\n",
    "    for word in sentence:\n",
    "        if word in inp_words:\n",
    "            inp_words[word] += 1\n",
    "        else:\n",
    "            inp_words[word] = 1\n",
    "            \n",
    "for sentence in tar:\n",
    "    for word in sentence:\n",
    "        if word in tar_words:\n",
    "            tar_words[word] += 1\n",
    "        else:\n",
    "            tar_words[word] = 1\n",
    "                    \n",
    "inp_vocab = len(inp_words)\n",
    "tar_vocab = len(tar_words)\n",
    "print(\"Number of Unique Words:\")\n",
    "print(inp_lang, ':', str(inp_vocab))\n",
    "print(tar_lang, ':', str(tar_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCA1636BxY_k"
   },
   "outputs": [],
   "source": [
    "# creating the 't'\n",
    "t = {}\n",
    "# usage: t[('inp_word', 'tar_word')] = probability of inp_Word given tar_word\n",
    "uniform = 1 / (inp_vocab * tar_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 128\n",
    "word_factor_max = 3\n",
    "\n",
    "fine_tune = 1\n",
    "has_converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2021-04-16 14:38:29.650312: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
       "2021-04-16 14:38:29.650439: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
       "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
       "                   [--path_prefix PATH] [--window_title TEXT]\n",
       "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
       "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: invalid choice: 'sem1COMP4801JupyterlogSMT-3_EngYue' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id = f\"SMT-3_EngYue\"\n",
    "log_dir = os.path.join(os.path.join(os.getcwd(), 'log'), run_id)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKUGRo00xY_p",
    "outputId": "ad2a8a33-2b25-4340-9525-a18ab4519552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Completed, Maximum Change: 0.8167420220033955\n",
      "Time: 8.92363715171814 secs\n",
      "Iteration 2 Completed, Maximum Change: 0.31770346346782335\n",
      "Time: 10.412647485733032 secs\n",
      "Iteration 3 Completed, Maximum Change: 0.16403717319027095\n",
      "Time: 10.193183183670044 secs\n",
      "Iteration 4 Completed, Maximum Change: 0.09379803735609438\n",
      "Time: 9.479170799255371 secs\n",
      "Iteration 5 Completed, Maximum Change: 0.054975635682816326\n",
      "Time: 10.419081211090088 secs\n",
      "Iteration 6 Completed, Maximum Change: 0.03181043541701345\n",
      "Time: 9.522835969924927 secs\n",
      "Iteration 7 Completed, Maximum Change: 0.021376901620688393\n",
      "Time: 12.199121713638306 secs\n",
      "Iteration 8 Completed, Maximum Change: 0.017734742353392396\n",
      "Time: 12.591800928115845 secs\n",
      "Iteration 9 Completed, Maximum Change: 0.015132331979161417\n",
      "Time: 10.876308917999268 secs\n",
      "Iteration 10 Completed, Maximum Change: 0.012491437365023034\n",
      "Time: 13.346911907196045 secs\n",
      "Iteration 11 Completed, Maximum Change: 0.011139501941502317\n",
      "Time: 10.734346389770508 secs\n",
      "Iteration 12 Completed, Maximum Change: 0.010625744958143446\n",
      "Time: 10.723040103912354 secs\n",
      "Iteration 13 Completed, Maximum Change: 0.0101063109100408\n",
      "Time: 9.945951223373413 secs\n",
      "Iteration 14 Completed, Maximum Change: 0.009599693778747864\n",
      "Time: 9.558382987976074 secs\n",
      "Iteration 15 Completed, Maximum Change: 0.009116010237158467\n",
      "Time: 8.387301921844482 secs\n",
      "Iteration 16 Completed, Maximum Change: 0.008659751908622698\n",
      "Time: 8.59763503074646 secs\n",
      "Iteration 17 Completed, Maximum Change: 0.008232047452133323\n",
      "Time: 8.771985054016113 secs\n",
      "Iteration 18 Completed, Maximum Change: 0.007832193728947695\n",
      "Time: 9.443031787872314 secs\n",
      "Iteration 19 Completed, Maximum Change: 0.007458534439019671\n",
      "Time: 10.068719625473022 secs\n",
      "Iteration 20 Completed, Maximum Change: 0.007108971903144468\n",
      "Time: 9.357142686843872 secs\n",
      "Iteration 21 Completed, Maximum Change: 0.006781283532879073\n",
      "Time: 12.330842971801758 secs\n",
      "Iteration 22 Completed, Maximum Change: 0.006473316134445711\n",
      "Time: 14.252772331237793 secs\n",
      "Iteration 23 Completed, Maximum Change: 0.006183093863378314\n",
      "Time: 17.716326475143433 secs\n",
      "Iteration 24 Completed, Maximum Change: 0.0059088665406595575\n",
      "Time: 9.14610242843628 secs\n",
      "Iteration 25 Completed, Maximum Change: 0.005649119912960798\n",
      "Time: 9.908931732177734 secs\n",
      "Iteration 26 Completed, Maximum Change: 0.005402563435807278\n",
      "Time: 8.890130996704102 secs\n",
      "Iteration 27 Completed, Maximum Change: 0.0051681060562286385\n",
      "Time: 9.13949465751648 secs\n",
      "Iteration 28 Completed, Maximum Change: -1\n",
      "Time: 11.427058219909668 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_iters = 0\n",
    "\n",
    "while n_iters < max_iters and has_converged == False:\n",
    "    start = time.time()\n",
    "    has_converged = True\n",
    "    max_change = -1\n",
    "\n",
    "    n_iters += 1\n",
    "    count = {}\n",
    "    total = {}\n",
    "    for index in range(train_size):\n",
    "        s_total = {}\n",
    "        for inp_word in inp[index]:\n",
    "            s_total[inp_word] = 0\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in t:\n",
    "                    t[(inp_word, tar_word)] = uniform\n",
    "                s_total[inp_word] += t[(inp_word, tar_word)]\n",
    "\n",
    "        for inp_word in inp[index]:\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in count:\n",
    "                    count[(inp_word, tar_word)] = 0\n",
    "                count[(inp_word, tar_word)] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "                if tar_word not in total:\n",
    "                    total[tar_word] = 0\n",
    "                total[tar_word] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "    # estimating the probabilities\n",
    "\n",
    "    if fine_tune == 0:\n",
    "      updated = {}\n",
    "      # train for all valid word pairs s.t count(inp_word, tar_word) > 0\n",
    "      for index in range(train_size):\n",
    "          for tar_word in tar[index]:\n",
    "              for inp_word in inp[index]:\n",
    "                  if (inp_word, tar_word) in updated:\n",
    "                      continue\n",
    "                  updated[(inp_word, tar_word)] = 1\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.01:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "\n",
    "    elif fine_tune == 1:\n",
    "      # train it only for 1000 most frequent words in English and Cantonese\n",
    "      n_tar_words = 0\n",
    "      updates = 0\n",
    "\n",
    "      for tar_word_tuples in sorted(tar_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "          tar_word = tar_word_tuples[0]\n",
    "          n_tar_words += 1\n",
    "          if n_tar_words > len(tar_words)/word_factor_max:\n",
    "              break\n",
    "          n_inp_words = 0\n",
    "          for inp_word_tuples in sorted(inp_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "              inp_word = inp_word_tuples[0]\n",
    "              n_inp_words += 1\n",
    "              if n_inp_words > len(inp_words)/word_factor_max:\n",
    "                  break\n",
    "              if (inp_word, tar_word) not in count or tar_word not in total:\n",
    "                  continue\n",
    "                  # assume in this case: t[(inp_word, tar_word)] = uniform\n",
    "              else:\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.005:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "                \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Change\", max_change, step=n_iters)\n",
    "\n",
    "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change) + '\\nTime: {} secs'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yaalpW64cA-",
    "outputId": "b0678767-12f0-42ed-ab2f-c4fd45fe9a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Applause', '掌'), 0.9881336679022288)\n",
      "(('Laughter', '笑'), 0.9085797120425466)\n",
      "(('or', '或'), 0.7550702662953516)\n",
      "(('Guitar', '吉'), 0.6683143794695754)\n",
      "(('cancer', '癌'), 0.6502667856144074)\n",
      "(('play', '玩'), 0.6461407553105636)\n",
      "(('you', '你'), 0.6193317785977169)\n",
      "(('I', '我'), 0.6178692285269027)\n",
      "(('because', '因'), 0.6081253636911552)\n",
      "(('and', '和'), 0.5975064908584243)\n",
      "(('Thank', '謝'), 0.5743202210446652)\n",
      "(('Laughter', '聲'), 0.5490333488497695)\n",
      "(('information', '息'), 0.5386506385176294)\n",
      "(('carousel', '迴'), 0.5300679072169148)\n",
      "(('know', '知'), 0.5219873095609776)\n",
      "(('serious', '嚴'), 0.5189755543882285)\n",
      "(('Bible', '聖'), 0.5088683785857239)\n",
      "(('like', '似'), 0.5025883870875981)\n",
      "(('and', '及'), 0.49385641183609047)\n",
      "(('and', '同'), 0.4825808239522696)\n",
      "(('So', '所'), 0.4806239643882358)\n",
      "(('change', '改'), 0.47830757340556057)\n",
      "(('to', '想'), 0.46520056381187375)\n",
      "(('love', '愛'), 0.45750719681837454)\n",
      "(('two', '兩'), 0.4540237443281956)\n",
      "(('more', '更'), 0.45377064065854594)\n",
      "(('we', '哋'), 0.4532283390680729)\n",
      "(('the', '界'), 0.4439041895147521)\n",
      "(('brain', '腦'), 0.44002478711918475)\n",
      "(('social', '社'), 0.4309559942864186)\n",
      "(('health', '健'), 0.430021478613315)\n",
      "(('But', '但'), 0.4299661746836663)\n",
      "(('the', '面'), 0.4297088161800358)\n",
      "(('media', '媒'), 0.4255499917295664)\n",
      "(('to', '去'), 0.425031374487314)\n",
      "(('but', '但'), 0.4245787812200853)\n",
      "(('young', '輕'), 0.41523051728113003)\n",
      "(('to', '要'), 0.4148708935800942)\n",
      "(('pain', '痛'), 0.41405729580260825)\n",
      "(('three', '三'), 0.41397670017479776)\n"
     ]
    }
   ],
   "source": [
    "# displaying the most confident translation pairs\n",
    "limit = 40\n",
    "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "  print(element)\n",
    "  limit -= 1\n",
    "  if limit <= 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DpMllrHFDYV-"
   },
   "outputs": [],
   "source": [
    "# saving the translation model\n",
    "file = open(\"translation_model.pkl\",\"wb\")\n",
    "pickle.dump(t, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4wQ0a-EARrIy"
   },
   "outputs": [],
   "source": [
    "# using the model trained until convergence\n",
    "# to use a saved model\n",
    "model_name = \"translation_model.pkl\"\n",
    "pickle_in = open(model_name,\"rb\")\n",
    "t = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yaz1CgJoCllZ"
   },
   "outputs": [],
   "source": [
    "I = {}\n",
    "for index in range(train_size):\n",
    "    for inp_id in range(len(inp[index])):\n",
    "        length = len(inp[index])\n",
    "        if length not in I:\n",
    "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
    "        for tar_id in range(len(tar[index])):\n",
    "            if (tar_id - inp_id) not in I[length]:\n",
    "                I[length][(tar_id - inp_id)] = [t[(inp[index][inp_id], tar[index][tar_id])], 1]\n",
    "            else:\n",
    "                I[length][(tar_id - inp_id)][0] += t[(inp[index][inp_id], tar[index][tar_id])]\n",
    "                I[length][(tar_id - inp_id)][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh6ogFdO2r_-",
    "outputId": "0d071c8f-b63d-407b-9d3a-c13e8308f2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "# viewing the available sentence lengths encountered during training\n",
    "sentence_lengths = []\n",
    "for key in I.keys():\n",
    "    if key not in sentence_lengths:\n",
    "        sentence_lengths.append(key)\n",
    "sentence_lengths.sort()\n",
    "print(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1CNo6HGz6lqK"
   },
   "outputs": [],
   "source": [
    "# computing the alignment probabilities\n",
    "# p[I][tar_id - inp_id] = p(i | i', I)\n",
    "\n",
    "p = {}\n",
    "for key in I.keys():\n",
    "    p[key] = {}\n",
    "    sum_val = 0\n",
    "    for diff in I[key].keys():\n",
    "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
    "        sum_val += p[key][diff]\n",
    "    for diff in p[key].keys():\n",
    "        p[key][diff] /= sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWI7HCoI81Qa",
    "outputId": "543de11e-d4e9-4b17-d13f-262a28b78541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 14\n",
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 13\n"
     ]
    }
   ],
   "source": [
    "for index in range(train_size):\n",
    "    length_inp = len(inp[index])\n",
    "    length_tar = len(tar[index])\n",
    "    if length_tar - length_inp > 10 and length_inp == 1:\n",
    "        print(\"Length of English Sentence:\", str(length_inp))\n",
    "        print(\"Length of Cantonese Sentence:\", str(length_tar))\n",
    "        \n",
    "# there exists an English sentence with one token s.t the Cantonese translation contains 19 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yqzyU-2g9_jk"
   },
   "outputs": [],
   "source": [
    "# computing initial transitions\n",
    "init = {}\n",
    "for length in p:\n",
    "    max_prob = -1\n",
    "    max_jump = 0\n",
    "    for key in p[length].keys():\n",
    "        if p[length][key] > max_prob:\n",
    "            max_prob = p[length][key]\n",
    "            max_jump = key\n",
    "    init[length] = max_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlM2-MzmExbc",
    "outputId": "c3791846-4116-4905-88db-6d78866b792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Bigrams: 27343\n",
      "Number of Unique Unigrams: 2705\n"
     ]
    }
   ],
   "source": [
    "# computing the transition probabilities for Cantonese\n",
    "bigrams = {}\n",
    "unigrams = {}\n",
    "\n",
    "# training on the train_set\n",
    "def model(dataset_size, dataset_name):\n",
    "    global bigrams\n",
    "    global unigrams\n",
    "    for index in range(dataset_size):\n",
    "        token_A = ''\n",
    "        for tar_token in tar[index]:\n",
    "            if tar_token not in unigrams:\n",
    "                unigrams[tar_token] = 1\n",
    "            else:\n",
    "                unigrams[tar_token] += 1\n",
    "            \n",
    "            token_B = tar_token\n",
    "            if (token_A, token_B) not in bigrams:\n",
    "                bigrams[(token_A, token_B)] = 1\n",
    "            else:\n",
    "                bigrams[(token_A, token_B)] += 1\n",
    "            token_A = token_B\n",
    "\n",
    "model(train_size, 'tar_train')\n",
    "\n",
    "bigram_count = len(bigrams)\n",
    "unigram_count = len(unigrams)\n",
    "print(\"Number of Unique Bigrams:\", bigram_count)\n",
    "print(\"Number of Unique Unigrams:\", unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AF98ePawMnaE",
    "outputId": "5f2fd7f7-3702-4fb0-807a-ee332d5ab102"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "computed_sentences = []\n",
    "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
    "null_BLEU_count = 0\n",
    "\n",
    "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
    "\n",
    "def find_translation(inp_token):\n",
    "    for element in sorted_t:\n",
    "        if element[0][0].lower() == inp_token:\n",
    "            return element[0][1]\n",
    "    return \"\"\n",
    "\n",
    "def get_prob(seq):\n",
    "    # bigram language model with laplace smoothing and backoff\n",
    "    if len(seq) < 2 or len(seq) > 10:\n",
    "        return 1\n",
    "    score = 0\n",
    "    token_A = ''\n",
    "    for tar_token in seq:\n",
    "        token_B = tar_token\n",
    "        if (token_A, token_B) not in bigrams:\n",
    "            if token_B not in unigrams:\n",
    "                continue\n",
    "            else:\n",
    "                score += unigrams[token_B] / unigram_count\n",
    "        else:\n",
    "            base_token_count = 0\n",
    "            if token_A in unigrams:\n",
    "                base_token_count = unigrams[token_A]\n",
    "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
    "        token_A = token_B\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 / 1696 in 8.7995445728302 s\n",
      "Progress: 100 / 1696 in 6.7104222774505615 s\n",
      "Progress: 150 / 1696 in 11.891496181488037 s\n",
      "Progress: 200 / 1696 in 9.858823299407959 s\n",
      "Progress: 250 / 1696 in 6.611072063446045 s\n",
      "Progress: 300 / 1696 in 8.347437620162964 s\n",
      "Progress: 350 / 1696 in 12.71769404411316 s\n",
      "Progress: 400 / 1696 in 10.625991821289062 s\n",
      "Progress: 450 / 1696 in 13.843483924865723 s\n",
      "Progress: 500 / 1696 in 10.529065132141113 s\n",
      "Progress: 550 / 1696 in 9.419617652893066 s\n",
      "Progress: 600 / 1696 in 9.650025367736816 s\n",
      "Progress: 650 / 1696 in 15.727373361587524 s\n",
      "Progress: 700 / 1696 in 11.790697574615479 s\n",
      "Progress: 750 / 1696 in 11.575767278671265 s\n",
      "Progress: 800 / 1696 in 18.382888793945312 s\n",
      "Progress: 850 / 1696 in 16.22527551651001 s\n",
      "Progress: 900 / 1696 in 11.688991785049438 s\n",
      "Progress: 950 / 1696 in 15.606330156326294 s\n",
      "Progress: 1000 / 1696 in 15.660196304321289 s\n",
      "Progress: 1050 / 1696 in 11.319407224655151 s\n",
      "Progress: 1100 / 1696 in 6.856293439865112 s\n",
      "Progress: 1150 / 1696 in 16.220665454864502 s\n",
      "Progress: 1200 / 1696 in 10.506202697753906 s\n",
      "Progress: 1250 / 1696 in 15.13141393661499 s\n",
      "Progress: 1300 / 1696 in 17.522955656051636 s\n",
      "Progress: 1350 / 1696 in 12.824511766433716 s\n",
      "Progress: 1400 / 1696 in 11.446374654769897 s\n",
      "Progress: 1450 / 1696 in 8.802738666534424 s\n",
      "Progress: 1500 / 1696 in 11.819032430648804 s\n",
      "Progress: 1550 / 1696 in 12.271673202514648 s\n",
      "Progress: 1600 / 1696 in 13.712206840515137 s\n",
      "Progress: 1650 / 1696 in 15.93384575843811 s\n",
      "\n",
      "ToTal: 1696 / 1696 in 407.45522570610046 s\n"
     ]
    }
   ],
   "source": [
    "TotalTime = time.time()\n",
    "start = time.time()\n",
    "\n",
    "ori = []\n",
    "ref = []\n",
    "can = []\n",
    "num_test = 0\n",
    "\n",
    "for index in range(test_size):\n",
    "\n",
    "    translated_words = []\n",
    "    for inp_token in inp_test[index]:\n",
    "        translation = find_translation(inp_token)\n",
    "        if translation != \"\":\n",
    "            translated_words.append(translation)\n",
    "    perm = permutations(translated_words)\n",
    "    best_seq = translated_words\n",
    "    best_prob = -1\n",
    "\n",
    "    if len(inp_test[index]) < 10 and len(inp_test[index]) > 2:\n",
    "        for seq in perm:            \n",
    "            prob = get_prob(seq)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_seq = seq\n",
    "            \n",
    "    ori.append(inp_test[index])\n",
    "    ref.append(tar_test[index])\n",
    "    can.append(best_seq)\n",
    "    num_test += 1\n",
    "    \n",
    "    if num_test % 50 == 0:\n",
    "        print(f'Progress: {num_test} / {test_size} in {time.time()-start} s')\n",
    "        start = time.time()\n",
    "        \n",
    "print(f'\\nToTal: {num_test} / {test_size} in {time.time()-TotalTime} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'are', 'coming', 'online']\n",
      "['正', '逐', '漸', '可', '於', '網', '上', '取', '得']\n",
      "('處', '規', '禮', '錄')\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import random\n",
    "ran = random.randint(1, len(ref))\n",
    "print(ori[ran])\n",
    "print(ref[ran])\n",
    "print(can[ran])\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "print('BLEU:', sentence_bleu(ref[ran], can[ran], smoothing_function=smoothie)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcR-JpzNT8gp",
    "outputId": "332fdf47-06f4-4e1b-8772-ba6b0357491a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-c: 0.18173735767640092\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based and average score\n",
    "score = 0\n",
    "# for i in range(len(ref)):\n",
    "#     r = ref[i]\n",
    "#     c = can[i]\n",
    "#     score += sentence_bleu(r, c, smoothing_function=smoothie)*100\n",
    "# print('BLEU-s:', score/len(ref))\n",
    "\n",
    "# Corpus based, summing all nominator and denominator before division\n",
    "# r = [[r.split()] for r in ref]\n",
    "# c = [c.split() for c in can]\n",
    "score = corpus_bleu(ref, can, smoothing_function=smoothie)*100\n",
    "print('BLEU-c:', score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SMT_English_to_Hindi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
