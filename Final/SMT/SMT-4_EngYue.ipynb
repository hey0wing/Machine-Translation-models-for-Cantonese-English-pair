{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u7y4if32xY-g"
   },
   "outputs": [],
   "source": [
    "# Statistical Machine Translation System\n",
    "# English to Cantonese\n",
    "\n",
    "# IBM Model 1 for Word Translation Task\n",
    "# Word Alignment based on Relative Positions\n",
    "# Bi-gram Language Modelling with Laplace Smoothing and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MsJzVKnIcb",
    "outputId": "74abecec-ed46-49f3-fee7-caf69e7d48ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hey0\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2cVVv4e0VoF0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aKVfJuwSxY-w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...</td>\n",
       "      <td>I was not allowed to touch or eat pickles I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>佢地扮怪面嚇人</td>\n",
       "      <td>They were making scary faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>唔會搞到我哋變得邪惡女人型變成自由黨人咩</td>\n",
       "      <td>turn us into godless sissy liberals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呢個模式可以清晰令您瞭解佢哋</td>\n",
       "      <td>So it spells those out in very clean terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>幾多萬億掌聲</td>\n",
       "      <td>How many trillions Applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0  唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...   \n",
       "1                                            佢地扮怪面嚇人   \n",
       "2                               唔會搞到我哋變得邪惡女人型變成自由黨人咩   \n",
       "3                                     呢個模式可以清晰令您瞭解佢哋   \n",
       "4                                             幾多萬億掌聲   \n",
       "\n",
       "                                                 eng  \n",
       "0  I was not allowed to touch or eat pickles I wa...  \n",
       "1                       They were making scary faces  \n",
       "2                turn us into godless sissy liberals  \n",
       "3         So it spells those out in very clean terms  \n",
       "4                        How many trillions Applause  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "all_csv = glob.glob(os.getcwd() + \"/train.csv\")  \n",
    "\n",
    "df_test = pd.read_csv(os.getcwd() + \"/test.csv\", sep='\\t', encoding='utf-8')  \n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, sep='\\t', encoding='utf-8') for f in all_csv)\n",
    "df = pd.concat(df_from_each_file)\n",
    "\n",
    "# Check for null\n",
    "df[df['yue'].isnull()]\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "YueChar = True\n",
    "# Delete spaces between n-gram in Cantonese\n",
    "# Perform Character based tokenization in Cantonese\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].str.replace(r' ', '')\n",
    "    df_test['yue'] = df_test['yue'].str.replace(r' ', '')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我 相 信 主</td>\n",
       "      <td>I believe the almighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友</td>\n",
       "      <td>For too long now children with developmental d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一 般 會 遇 到 兩 種 反 應</td>\n",
       "      <td>I have two kinds of reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...</td>\n",
       "      <td>But think about this The IMF official is hardl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>佢 將 呢 個 病 毒 傳 畀 BB</td>\n",
       "      <td>She passes that virus on to baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0                                            我 相 信 主   \n",
       "1                          好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友   \n",
       "2                                  一 般 會 遇 到 兩 種 反 應   \n",
       "3  但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...   \n",
       "4                                 佢 將 呢 個 病 毒 傳 畀 BB   \n",
       "\n",
       "                                                 eng  \n",
       "0                             I believe the almighty  \n",
       "1  For too long now children with developmental d...  \n",
       "2                      I have two kinds of reactions  \n",
       "3  But think about this The IMF official is hardl...  \n",
       "4                   She passes that virus on to baby  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "def spliteKeyWord(str):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "    df_test['yue'] = df_test['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['They', 'were', 'making', 'scary', 'faces'],\n",
       "  ['turn', 'us', 'into', 'godless', 'sissy', 'liberals'],\n",
       "  ['So', 'it', 'spells', 'those', 'out', 'in', 'very', 'clean', 'terms']],\n",
       " [['佢', '地', '扮', '怪', '面', '嚇', '人'],\n",
       "  ['唔',\n",
       "   '會',\n",
       "   '搞',\n",
       "   '到',\n",
       "   '我',\n",
       "   '哋',\n",
       "   '變',\n",
       "   '得',\n",
       "   '邪',\n",
       "   '惡',\n",
       "   '女',\n",
       "   '人',\n",
       "   '型',\n",
       "   '變',\n",
       "   '成',\n",
       "   '自',\n",
       "   '由',\n",
       "   '黨',\n",
       "   '人',\n",
       "   '咩'],\n",
       "  ['呢', '個', '模', '式', '可', '以', '清', '晰', '令', '您', '瞭', '解', '佢', '哋']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "inp_lang = 'eng'\n",
    "tar_lang = 'yue'\n",
    "\n",
    "def df_filter(df):\n",
    "    return df[\n",
    "        df['yue'].apply(lambda x: len(x.split())<MAX_LENGTH) &\n",
    "        df['eng'].apply(lambda x: len(x.split())<MAX_LENGTH) \n",
    "    ]\n",
    "df = df_filter(df)\n",
    "df_test = df_filter(df_test)\n",
    "\n",
    "inp = [i.split() for i in df[inp_lang].to_list()]\n",
    "tar = [i.split() for i in df[tar_lang].to_list()]\n",
    "inp_test = [i.split() for i in df_test[inp_lang].to_list()]\n",
    "tar_test = [i.split() for i in df_test[tar_lang].to_list()]\n",
    "inp[:3], tar[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIZNBPNAxY_S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 1696)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(inp)\n",
    "test_size = len(inp_test)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VMMiv4BxY_c",
    "outputId": "589049e5-2913-4453-d45f-0465ddcd9683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:\n",
      "eng : 6219\n",
      "yue : 2705\n"
     ]
    }
   ],
   "source": [
    "inp_words = {}\n",
    "tar_words = {}\n",
    "\n",
    "for sentence in inp:\n",
    "    for word in sentence:\n",
    "        if word in inp_words:\n",
    "            inp_words[word] += 1\n",
    "        else:\n",
    "            inp_words[word] = 1\n",
    "            \n",
    "for sentence in tar:\n",
    "    for word in sentence:\n",
    "        if word in tar_words:\n",
    "            tar_words[word] += 1\n",
    "        else:\n",
    "            tar_words[word] = 1\n",
    "                    \n",
    "inp_vocab = len(inp_words)\n",
    "tar_vocab = len(tar_words)\n",
    "print(\"Number of Unique Words:\")\n",
    "print(inp_lang, ':', str(inp_vocab))\n",
    "print(tar_lang, ':', str(tar_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCA1636BxY_k"
   },
   "outputs": [],
   "source": [
    "# creating the 't'\n",
    "t = {}\n",
    "# usage: t[('inp_word', 'tar_word')] = probability of inp_Word given tar_word\n",
    "uniform = 1 / (inp_vocab * tar_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 128\n",
    "word_factor_max = 4\n",
    "\n",
    "fine_tune = 1\n",
    "has_converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2021-04-16 05:32:01.937073: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
       "2021-04-16 05:32:01.937209: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
       "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
       "                   [--path_prefix PATH] [--window_title TEXT]\n",
       "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
       "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: invalid choice: 'sem1COMP4801JupyterlogSMT-4_EngYue' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id = f\"SMT-4_EngYue\"\n",
    "log_dir = os.path.join(os.path.join(os.getcwd(), 'log'), run_id)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKUGRo00xY_p",
    "outputId": "ad2a8a33-2b25-4340-9525-a18ab4519552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Completed, Maximum Change: 0.10061728323390656\n",
      "Time: 14.505329608917236 secs\n",
      "Iteration 2 Completed, Maximum Change: 0.059777312117207104\n",
      "Time: 14.042763948440552 secs\n",
      "Iteration 3 Completed, Maximum Change: 0.038890445614923885\n",
      "Time: 11.707644939422607 secs\n",
      "Iteration 4 Completed, Maximum Change: 0.031511048716855894\n",
      "Time: 14.11092209815979 secs\n",
      "Iteration 5 Completed, Maximum Change: 0.026656764333911676\n",
      "Time: 13.092541456222534 secs\n",
      "Iteration 6 Completed, Maximum Change: 0.023829517510665296\n",
      "Time: 11.382084608078003 secs\n",
      "Iteration 7 Completed, Maximum Change: 0.02870980011454298\n",
      "Time: 16.10074019432068 secs\n",
      "Iteration 8 Completed, Maximum Change: 0.028258624414520178\n",
      "Time: 13.923482656478882 secs\n",
      "Iteration 9 Completed, Maximum Change: 0.02338470113218183\n",
      "Time: 12.678022623062134 secs\n",
      "Iteration 10 Completed, Maximum Change: 0.017378590410586287\n",
      "Time: 11.37423062324524 secs\n",
      "Iteration 11 Completed, Maximum Change: 0.012408867866056161\n",
      "Time: 12.32548189163208 secs\n",
      "Iteration 12 Completed, Maximum Change: 0.009916326802802827\n",
      "Time: 12.870746850967407 secs\n",
      "Iteration 13 Completed, Maximum Change: 0.009424127632309987\n",
      "Time: 11.210137128829956 secs\n",
      "Iteration 14 Completed, Maximum Change: 0.008969306047253212\n",
      "Time: 11.286297082901001 secs\n",
      "Iteration 15 Completed, Maximum Change: 0.008546344826239033\n",
      "Time: 11.551670551300049 secs\n",
      "Iteration 16 Completed, Maximum Change: 0.008150768434376648\n",
      "Time: 11.223557949066162 secs\n",
      "Iteration 17 Completed, Maximum Change: 0.007779012201101471\n",
      "Time: 11.417449712753296 secs\n",
      "Iteration 18 Completed, Maximum Change: 0.007428241376469968\n",
      "Time: 11.340499877929688 secs\n",
      "Iteration 19 Completed, Maximum Change: 0.0070961880987739645\n",
      "Time: 11.328322172164917 secs\n",
      "Iteration 20 Completed, Maximum Change: 0.006781021353318761\n",
      "Time: 11.369102954864502 secs\n",
      "Iteration 21 Completed, Maximum Change: 0.006481246720992179\n",
      "Time: 11.536829710006714 secs\n",
      "Iteration 22 Completed, Maximum Change: 0.006195629043814224\n",
      "Time: 11.460473775863647 secs\n",
      "Iteration 23 Completed, Maximum Change: 0.005923132004543918\n",
      "Time: 11.409135580062866 secs\n",
      "Iteration 24 Completed, Maximum Change: 0.005662870558914332\n",
      "Time: 11.503230094909668 secs\n",
      "Iteration 25 Completed, Maximum Change: 0.00541407463007014\n",
      "Time: 13.260015726089478 secs\n",
      "Iteration 26 Completed, Maximum Change: 0.005176064228811408\n",
      "Time: 11.536048889160156 secs\n",
      "Iteration 27 Completed, Maximum Change: -1\n",
      "Time: 11.711426973342896 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_iters = 0\n",
    "\n",
    "while n_iters < max_iters and has_converged == False:\n",
    "    start = time.time()\n",
    "    has_converged = True\n",
    "    max_change = -1\n",
    "\n",
    "    n_iters += 1\n",
    "    count = {}\n",
    "    total = {}\n",
    "    for index in range(train_size):\n",
    "        s_total = {}\n",
    "        for inp_word in inp[index]:\n",
    "            s_total[inp_word] = 0\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in t:\n",
    "                    t[(inp_word, tar_word)] = uniform\n",
    "                s_total[inp_word] += t[(inp_word, tar_word)]\n",
    "\n",
    "        for inp_word in inp[index]:\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in count:\n",
    "                    count[(inp_word, tar_word)] = 0\n",
    "                count[(inp_word, tar_word)] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "                if tar_word not in total:\n",
    "                    total[tar_word] = 0\n",
    "                total[tar_word] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "    # estimating the probabilities\n",
    "\n",
    "    if fine_tune == 0:\n",
    "      updated = {}\n",
    "      # train for all valid word pairs s.t count(inp_word, tar_word) > 0\n",
    "      for index in range(train_size):\n",
    "          for tar_word in tar[index]:\n",
    "              for inp_word in inp[index]:\n",
    "                  if (inp_word, tar_word) in updated:\n",
    "                      continue\n",
    "                  updated[(inp_word, tar_word)] = 1\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.01:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "\n",
    "    elif fine_tune == 1:\n",
    "      # train it only for 1000 most frequent words in English and Cantonese\n",
    "      n_tar_words = 0\n",
    "      updates = 0\n",
    "\n",
    "      for tar_word_tuples in sorted(tar_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "          tar_word = tar_word_tuples[0]\n",
    "          n_tar_words += 1\n",
    "          if n_tar_words > len(tar_words)/word_factor_max:\n",
    "              break\n",
    "          n_inp_words = 0\n",
    "          for inp_word_tuples in sorted(inp_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "              inp_word = inp_word_tuples[0]\n",
    "              n_inp_words += 1\n",
    "              if n_inp_words > len(inp_words)/word_factor_max:\n",
    "                  break\n",
    "              if (inp_word, tar_word) not in count or tar_word not in total:\n",
    "                  continue\n",
    "                  # assume in this case: t[(inp_word, tar_word)] = uniform\n",
    "              else:\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.005:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "                \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Change\", max_change, step=n_iters)\n",
    "\n",
    "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change) + '\\nTime: {} secs'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yaalpW64cA-",
    "outputId": "b0678767-12f0-42ed-ab2f-c4fd45fe9a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Applause', '掌'), 0.9815791241298171)\n",
      "(('Guitar', '吉'), 0.9688366279043082)\n",
      "(('Laughter', '笑'), 0.9393978572985704)\n",
      "(('or', '或'), 0.817416290652338)\n",
      "(('fell', '跌'), 0.7248889905258226)\n",
      "(('cancer', '癌'), 0.7102642291999152)\n",
      "(('play', '玩'), 0.6940455900997853)\n",
      "(('and', '和'), 0.6539258955800603)\n",
      "(('I', '我'), 0.6445171903608035)\n",
      "(('because', '因'), 0.634531054646612)\n",
      "(('you', '你'), 0.6324926858713258)\n",
      "(('and', '及'), 0.6134175614713324)\n",
      "(('like', '似'), 0.6084316639120902)\n",
      "(('serious', '嚴'), 0.6048784165103847)\n",
      "(('information', '息'), 0.6043693153370403)\n",
      "(('Applause', '鼓'), 0.5975765159157658)\n",
      "(('apartment', '寓'), 0.5907856128049263)\n",
      "(('Thank', '謝'), 0.5896039851766257)\n",
      "(('Bible', '聖'), 0.5879872359032474)\n",
      "(('green', '綠'), 0.5877173906474982)\n",
      "(('love', '愛'), 0.5634707525522883)\n",
      "(('revolution', '革'), 0.5556046725470167)\n",
      "(('carousel', '迴'), 0.5516193436696922)\n",
      "(('know', '知'), 0.549194185436135)\n",
      "(('Jesus', '穌'), 0.5424091988607987)\n",
      "(('EEG', 'EEG'), 0.53769647313886)\n",
      "(('two', '兩'), 0.536421722655367)\n",
      "(('heart', '臟'), 0.5249796732300256)\n",
      "(('and', '縱'), 0.5164457655834981)\n",
      "(('her', '腕'), 0.51572323590549)\n",
      "(('Laughter', '聲'), 0.5152784764506555)\n",
      "(('Africa', '洲'), 0.5150956816687678)\n",
      "(('Paris', '黎'), 0.5137294052446496)\n",
      "(('autism', '閉'), 0.5096495123602399)\n",
      "(('carbon', '碳'), 0.5041818882824164)\n",
      "(('So', '所'), 0.49957989587925256)\n",
      "(('more', '更'), 0.4993468495982924)\n",
      "(('and', '同'), 0.4990327691187972)\n",
      "(('and', '埋'), 0.4936421626970459)\n",
      "(('TED', 'TED'), 0.4935124801602544)\n"
     ]
    }
   ],
   "source": [
    "# displaying the most confident translation pairs\n",
    "limit = 40\n",
    "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "  print(element)\n",
    "  limit -= 1\n",
    "  if limit <= 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DpMllrHFDYV-"
   },
   "outputs": [],
   "source": [
    "# saving the translation model\n",
    "file = open(\"translation_model.pkl\",\"wb\")\n",
    "pickle.dump(t, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4wQ0a-EARrIy"
   },
   "outputs": [],
   "source": [
    "# using the model trained until convergence\n",
    "# to use a saved model\n",
    "model_name = \"translation_model.pkl\"\n",
    "pickle_in = open(model_name,\"rb\")\n",
    "t = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yaz1CgJoCllZ"
   },
   "outputs": [],
   "source": [
    "I = {}\n",
    "for index in range(train_size):\n",
    "    for inp_id in range(len(inp[index])):\n",
    "        length = len(inp[index])\n",
    "        if length not in I:\n",
    "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
    "        for tar_id in range(len(tar[index])):\n",
    "            if (tar_id - inp_id) not in I[length]:\n",
    "                I[length][(tar_id - inp_id)] = [t[(inp[index][inp_id], tar[index][tar_id])], 1]\n",
    "            else:\n",
    "                I[length][(tar_id - inp_id)][0] += t[(inp[index][inp_id], tar[index][tar_id])]\n",
    "                I[length][(tar_id - inp_id)][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh6ogFdO2r_-",
    "outputId": "0d071c8f-b63d-407b-9d3a-c13e8308f2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "# viewing the available sentence lengths encountered during training\n",
    "sentence_lengths = []\n",
    "for key in I.keys():\n",
    "    if key not in sentence_lengths:\n",
    "        sentence_lengths.append(key)\n",
    "sentence_lengths.sort()\n",
    "print(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1CNo6HGz6lqK"
   },
   "outputs": [],
   "source": [
    "# computing the alignment probabilities\n",
    "# p[I][tar_id - inp_id] = p(i | i', I)\n",
    "\n",
    "p = {}\n",
    "for key in I.keys():\n",
    "    p[key] = {}\n",
    "    sum_val = 0\n",
    "    for diff in I[key].keys():\n",
    "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
    "        sum_val += p[key][diff]\n",
    "    for diff in p[key].keys():\n",
    "        p[key][diff] /= sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWI7HCoI81Qa",
    "outputId": "543de11e-d4e9-4b17-d13f-262a28b78541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 14\n",
      "Length of English Sentence: 1\n",
      "Length of Cantonese Sentence: 13\n"
     ]
    }
   ],
   "source": [
    "for index in range(train_size):\n",
    "    length_inp = len(inp[index])\n",
    "    length_tar = len(tar[index])\n",
    "    if length_tar - length_inp > 10 and length_inp == 1:\n",
    "        print(\"Length of English Sentence:\", str(length_inp))\n",
    "        print(\"Length of Cantonese Sentence:\", str(length_tar))\n",
    "        \n",
    "# there exists an English sentence with one token s.t the Cantonese translation contains 19 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yqzyU-2g9_jk"
   },
   "outputs": [],
   "source": [
    "# computing initial transitions\n",
    "init = {}\n",
    "for length in p:\n",
    "    max_prob = -1\n",
    "    max_jump = 0\n",
    "    for key in p[length].keys():\n",
    "        if p[length][key] > max_prob:\n",
    "            max_prob = p[length][key]\n",
    "            max_jump = key\n",
    "    init[length] = max_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlM2-MzmExbc",
    "outputId": "c3791846-4116-4905-88db-6d78866b792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Bigrams: 27343\n",
      "Number of Unique Unigrams: 2705\n"
     ]
    }
   ],
   "source": [
    "# computing the transition probabilities for Cantonese\n",
    "bigrams = {}\n",
    "unigrams = {}\n",
    "\n",
    "# training on the train_set\n",
    "def model(dataset_size, dataset_name):\n",
    "    global bigrams\n",
    "    global unigrams\n",
    "    for index in range(dataset_size):\n",
    "        token_A = ''\n",
    "        for tar_token in tar[index]:\n",
    "            if tar_token not in unigrams:\n",
    "                unigrams[tar_token] = 1\n",
    "            else:\n",
    "                unigrams[tar_token] += 1\n",
    "            \n",
    "            token_B = tar_token\n",
    "            if (token_A, token_B) not in bigrams:\n",
    "                bigrams[(token_A, token_B)] = 1\n",
    "            else:\n",
    "                bigrams[(token_A, token_B)] += 1\n",
    "            token_A = token_B\n",
    "\n",
    "model(train_size, 'tar_train')\n",
    "\n",
    "bigram_count = len(bigrams)\n",
    "unigram_count = len(unigrams)\n",
    "print(\"Number of Unique Bigrams:\", bigram_count)\n",
    "print(\"Number of Unique Unigrams:\", unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AF98ePawMnaE",
    "outputId": "5f2fd7f7-3702-4fb0-807a-ee332d5ab102"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "computed_sentences = []\n",
    "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
    "null_BLEU_count = 0\n",
    "\n",
    "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
    "\n",
    "def find_translation(inp_token):\n",
    "    for element in sorted_t:\n",
    "        if element[0][0].lower() == inp_token:\n",
    "            return element[0][1]\n",
    "    return \"\"\n",
    "\n",
    "def get_prob(seq):\n",
    "    # bigram language model with laplace smoothing and backoff\n",
    "    if len(seq) < 2 or len(seq) > 10:\n",
    "        return 1\n",
    "    score = 0\n",
    "    token_A = ''\n",
    "    for tar_token in seq:\n",
    "        token_B = tar_token\n",
    "        if (token_A, token_B) not in bigrams:\n",
    "            if token_B not in unigrams:\n",
    "                continue\n",
    "            else:\n",
    "                score += unigrams[token_B] / unigram_count\n",
    "        else:\n",
    "            base_token_count = 0\n",
    "            if token_A in unigrams:\n",
    "                base_token_count = unigrams[token_A]\n",
    "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
    "        token_A = token_B\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 / 1696 in 7.060065507888794 s\n",
      "Progress: 100 / 1696 in 6.184078693389893 s\n",
      "Progress: 150 / 1696 in 9.875119686126709 s\n",
      "Progress: 200 / 1696 in 8.030096769332886 s\n",
      "Progress: 250 / 1696 in 5.2630932331085205 s\n",
      "Progress: 300 / 1696 in 6.4435882568359375 s\n",
      "Progress: 350 / 1696 in 9.843735218048096 s\n",
      "Progress: 400 / 1696 in 5.851065635681152 s\n",
      "Progress: 450 / 1696 in 7.193043231964111 s\n",
      "Progress: 500 / 1696 in 7.778099298477173 s\n",
      "Progress: 550 / 1696 in 6.250096321105957 s\n",
      "Progress: 600 / 1696 in 6.352885007858276 s\n",
      "Progress: 650 / 1696 in 7.525693416595459 s\n",
      "Progress: 700 / 1696 in 6.998638391494751 s\n",
      "Progress: 750 / 1696 in 7.3835060596466064 s\n",
      "Progress: 800 / 1696 in 9.561376571655273 s\n",
      "Progress: 850 / 1696 in 8.418327569961548 s\n",
      "Progress: 900 / 1696 in 7.2670738697052 s\n",
      "Progress: 950 / 1696 in 8.114732265472412 s\n",
      "Progress: 1000 / 1696 in 7.295686721801758 s\n",
      "Progress: 1050 / 1696 in 7.541095495223999 s\n",
      "Progress: 1100 / 1696 in 4.974968671798706 s\n",
      "Progress: 1150 / 1696 in 7.369691610336304 s\n",
      "Progress: 1200 / 1696 in 6.68159294128418 s\n",
      "Progress: 1250 / 1696 in 9.161037683486938 s\n",
      "Progress: 1300 / 1696 in 8.53512978553772 s\n",
      "Progress: 1350 / 1696 in 7.8450493812561035 s\n",
      "Progress: 1400 / 1696 in 7.382106065750122 s\n",
      "Progress: 1450 / 1696 in 5.826125621795654 s\n",
      "Progress: 1500 / 1696 in 7.42948579788208 s\n",
      "Progress: 1550 / 1696 in 8.291613578796387 s\n",
      "Progress: 1600 / 1696 in 8.734499216079712 s\n",
      "Progress: 1650 / 1696 in 10.640114068984985 s\n",
      "\n",
      "ToTal: 1696 / 1696 in 255.11308789253235 s\n"
     ]
    }
   ],
   "source": [
    "TotalTime = time.time()\n",
    "start = time.time()\n",
    "\n",
    "ori = []\n",
    "ref = []\n",
    "can = []\n",
    "num_test = 0\n",
    "\n",
    "for index in range(test_size):\n",
    "\n",
    "    translated_words = []\n",
    "    for inp_token in inp_test[index]:\n",
    "        translation = find_translation(inp_token)\n",
    "        if translation != \"\":\n",
    "            translated_words.append(translation)\n",
    "    perm = permutations(translated_words)\n",
    "    best_seq = translated_words\n",
    "    best_prob = -1\n",
    "\n",
    "    if len(inp_test[index]) < 10 and len(inp_test[index]) > 2:\n",
    "        for seq in perm:            \n",
    "            prob = get_prob(seq)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_seq = seq\n",
    "            \n",
    "    ori.append(inp_test[index])\n",
    "    ref.append(tar_test[index])\n",
    "    can.append(best_seq)\n",
    "    num_test += 1\n",
    "    \n",
    "    if num_test % 50 == 0:\n",
    "        print(f'Progress: {num_test} / {test_size} in {time.time()-start} s')\n",
    "        start = time.time()\n",
    "        \n",
    "print(f'\\nToTal: {num_test} / {test_size} in {time.time()-TotalTime} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'time', 'the', 'doctor', 's', 'potent', 'potion', 'and', 'uncanny', 'instincts', 'became', 'known', 'throughout', 'the', 'land', 'He', 'grew', 'rich', 'and', 'famous', 'casting', 'off', 'the', 'hardships', 'of', 'his', 'early', 'life']\n",
      "['而', '契', '仔', '只', '可', '以', '就', '手', '旁', '觀', '憑', '住', '呢', '樽', '藥', '水', '同', '神', '奇', '嘅', '直', '覺', '契', '仔', '好', '快', '就', '成', '為', '遠', '近', '聞', '名', '嘅', '神', '醫']\n",
      "['次', '界', '醫', '墨', '和', '豆', '深', '麥', '界', '土', '長', '逐', '和', '過', '熄', '界', '標', '蠟', '右', '命']\n",
      "BLEU: 14.0817820723198\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import random\n",
    "ran = random.randint(1, len(ref))\n",
    "print(ori[ran])\n",
    "print(ref[ran])\n",
    "print(can[ran])\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "print('BLEU:', sentence_bleu(ref[ran], can[ran], smoothing_function=smoothie)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcR-JpzNT8gp",
    "outputId": "332fdf47-06f4-4e1b-8772-ba6b0357491a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-c: 0.1733453072847488\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based and average score\n",
    "score = 0\n",
    "# for i in range(len(ref)):\n",
    "#     r = ref[i]\n",
    "#     c = can[i]\n",
    "#     score += sentence_bleu(r, c, smoothing_function=smoothie)*100\n",
    "# print('BLEU-s:', score/len(ref))\n",
    "\n",
    "# Corpus based, summing all nominator and denominator before division\n",
    "# r = [[r.split()] for r in ref]\n",
    "# c = [c.split() for c in can]\n",
    "score = corpus_bleu(ref, can, smoothing_function=smoothie)*100\n",
    "print('BLEU-c:', score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SMT_English_to_Hindi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
