{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u7y4if32xY-g"
   },
   "outputs": [],
   "source": [
    "# Statistical Machine Translation System\n",
    "# English to Cantonese\n",
    "\n",
    "# IBM Model 1 for Word Translation Task\n",
    "# Word Alignment based on Relative Positions\n",
    "# Bi-gram Language Modelling with Laplace Smoothing and Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MsJzVKnIcb",
    "outputId": "74abecec-ed46-49f3-fee7-caf69e7d48ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hey0\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: regex in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hey0\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2cVVv4e0VoF0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aKVfJuwSxY-w"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...</td>\n",
       "      <td>I was not allowed to touch or eat pickles I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>佢地扮怪面嚇人</td>\n",
       "      <td>They were making scary faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>唔會搞到我哋變得邪惡女人型變成自由黨人咩</td>\n",
       "      <td>turn us into godless sissy liberals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>呢個模式可以清晰令您瞭解佢哋</td>\n",
       "      <td>So it spells those out in very clean terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>幾多萬億掌聲</td>\n",
       "      <td>How many trillions Applause</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0  唔準掂同食醃菜唔準坐梳化或者屋企人嘅床上每次經期完咗要洗床單就算床單無汚糟到他們認為我唔純潔...   \n",
       "1                                            佢地扮怪面嚇人   \n",
       "2                               唔會搞到我哋變得邪惡女人型變成自由黨人咩   \n",
       "3                                     呢個模式可以清晰令您瞭解佢哋   \n",
       "4                                             幾多萬億掌聲   \n",
       "\n",
       "                                                 eng  \n",
       "0  I was not allowed to touch or eat pickles I wa...  \n",
       "1                       They were making scary faces  \n",
       "2                turn us into godless sissy liberals  \n",
       "3         So it spells those out in very clean terms  \n",
       "4                        How many trillions Applause  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "all_csv = glob.glob(os.getcwd() + \"/train.csv\")  \n",
    "\n",
    "df_test = pd.read_csv(os.getcwd() + \"/test.csv\", sep='\\t', encoding='utf-8')  \n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, sep='\\t', encoding='utf-8') for f in all_csv)\n",
    "df = pd.concat(df_from_each_file)\n",
    "\n",
    "# Check for null\n",
    "df[df['yue'].isnull()]\n",
    "df = df.dropna()\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "YueChar = True\n",
    "# Delete spaces between n-gram in Cantonese\n",
    "# Perform Character based tokenization in Cantonese\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].str.replace(r' ', '')\n",
    "    df_test['yue'] = df_test['yue'].str.replace(r' ', '')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yue</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我 相 信 主</td>\n",
       "      <td>I believe the almighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友</td>\n",
       "      <td>For too long now children with developmental d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一 般 會 遇 到 兩 種 反 應</td>\n",
       "      <td>I have two kinds of reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...</td>\n",
       "      <td>But think about this The IMF official is hardl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>佢 將 呢 個 病 毒 傳 畀 BB</td>\n",
       "      <td>She passes that virus on to baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 yue  \\\n",
       "0                                            我 相 信 主   \n",
       "1                          好 耐 以 嚟 有 發 展 紊 亂 嘅 小 朋 友   \n",
       "2                                  一 般 會 遇 到 兩 種 反 應   \n",
       "3  但 再 諗 下 嗰 位 官 員 未 必 係 唯 一 睇 小 女 性 嘅 人 呢 種 偏 見 ...   \n",
       "4                                 佢 將 呢 個 病 毒 傳 畀 BB   \n",
       "\n",
       "                                                 eng  \n",
       "0                             I believe the almighty  \n",
       "1  For too long now children with developmental d...  \n",
       "2                      I have two kinds of reactions  \n",
       "3  But think about this The IMF official is hardl...  \n",
       "4                   She passes that virus on to baby  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "def spliteKeyWord(str):\n",
    "    regex = r\"[\\u4e00-\\ufaff]|[0-9]+|[a-zA-Z]+\\'*[a-z]*\"\n",
    "    matches = re.findall(regex, str, re.UNICODE)\n",
    "    return ' '.join(matches)\n",
    "\n",
    "if YueChar:\n",
    "    df['yue'] = df['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "    df_test['yue'] = df_test['yue'].apply(lambda x: spliteKeyWord(x))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['佢', '地', '扮', '怪', '面', '嚇', '人'],\n",
       "  ['唔',\n",
       "   '會',\n",
       "   '搞',\n",
       "   '到',\n",
       "   '我',\n",
       "   '哋',\n",
       "   '變',\n",
       "   '得',\n",
       "   '邪',\n",
       "   '惡',\n",
       "   '女',\n",
       "   '人',\n",
       "   '型',\n",
       "   '變',\n",
       "   '成',\n",
       "   '自',\n",
       "   '由',\n",
       "   '黨',\n",
       "   '人',\n",
       "   '咩'],\n",
       "  ['呢', '個', '模', '式', '可', '以', '清', '晰', '令', '您', '瞭', '解', '佢', '哋']],\n",
       " [['They', 'were', 'making', 'scary', 'faces'],\n",
       "  ['turn', 'us', 'into', 'godless', 'sissy', 'liberals'],\n",
       "  ['So', 'it', 'spells', 'those', 'out', 'in', 'very', 'clean', 'terms']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "inp_lang = 'yue'\n",
    "tar_lang = 'eng'\n",
    "\n",
    "def df_filter(df):\n",
    "    return df[\n",
    "        df['yue'].apply(lambda x: len(x.split())<MAX_LENGTH) &\n",
    "        df['eng'].apply(lambda x: len(x.split())<MAX_LENGTH) \n",
    "    ]\n",
    "df = df_filter(df)\n",
    "df_test = df_filter(df_test)\n",
    "\n",
    "inp = [i.split() for i in df[inp_lang].to_list()]\n",
    "tar = [i.split() for i in df[tar_lang].to_list()]\n",
    "inp_test = [i.split() for i in df_test[inp_lang].to_list()]\n",
    "tar_test = [i.split() for i in df_test[tar_lang].to_list()]\n",
    "inp[:3], tar[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIZNBPNAxY_S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 1696)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(inp)\n",
    "test_size = len(inp_test)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VMMiv4BxY_c",
    "outputId": "589049e5-2913-4453-d45f-0465ddcd9683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Words:\n",
      "yue : 2705\n",
      "eng : 6219\n"
     ]
    }
   ],
   "source": [
    "inp_words = {}\n",
    "tar_words = {}\n",
    "\n",
    "for sentence in inp:\n",
    "    for word in sentence:\n",
    "        if word in inp_words:\n",
    "            inp_words[word] += 1\n",
    "        else:\n",
    "            inp_words[word] = 1\n",
    "            \n",
    "for sentence in tar:\n",
    "    for word in sentence:\n",
    "        if word in tar_words:\n",
    "            tar_words[word] += 1\n",
    "        else:\n",
    "            tar_words[word] = 1\n",
    "                    \n",
    "inp_vocab = len(inp_words)\n",
    "tar_vocab = len(tar_words)\n",
    "print(\"Number of Unique Words:\")\n",
    "print(inp_lang, ':', str(inp_vocab))\n",
    "print(tar_lang, ':', str(tar_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCA1636BxY_k"
   },
   "outputs": [],
   "source": [
    "# creating the 't'\n",
    "t = {}\n",
    "# usage: t[('inp_word', 'tar_word')] = probability of inp_Word given tar_word\n",
    "uniform = 1 / (inp_vocab * tar_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 128\n",
    "word_factor_max = 1\n",
    "\n",
    "fine_tune = 1\n",
    "has_converged = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 2).\n",
       "Contents of stderr:\n",
       "2021-04-16 15:02:53.564243: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
       "2021-04-16 15:02:53.564409: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
       "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
       "                   [--host ADDR] [--bind_all] [--port PORT]\n",
       "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import]\n",
       "                   [--inspect] [--version_tb] [--tag TAG] [--event_file PATH]\n",
       "                   [--path_prefix PATH] [--window_title TEXT]\n",
       "                   [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
       "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
       "                   [--reload_multifile_inactive_secs SECONDS]\n",
       "                   [--generic_data TYPE]\n",
       "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
       "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
       "                   {serve,dev} ...\n",
       "tensorboard: error: invalid choice: 'sem1COMP4801JupyterlogSMT-1_YueEng' (choose from 'serve', 'dev')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_id = f\"SMT-1_YueEng\"\n",
    "log_dir = os.path.join(os.path.join(os.getcwd(), 'log'), run_id)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKUGRo00xY_p",
    "outputId": "ad2a8a33-2b25-4340-9525-a18ab4519552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Completed, Maximum Change: 0.9999999405554322\n",
      "Time: 57.48083782196045 secs\n",
      "Iteration 2 Completed, Maximum Change: 0.3352959289604691\n",
      "Time: 56.35574007034302 secs\n",
      "Iteration 3 Completed, Maximum Change: 0.23961967870026474\n",
      "Time: 56.25474500656128 secs\n",
      "Iteration 4 Completed, Maximum Change: 0.13242381627796806\n",
      "Time: 56.601805448532104 secs\n",
      "Iteration 5 Completed, Maximum Change: 0.09648838380079872\n",
      "Time: 56.20622396469116 secs\n",
      "Iteration 6 Completed, Maximum Change: 0.078783735870318\n",
      "Time: 56.36272668838501 secs\n",
      "Iteration 7 Completed, Maximum Change: 0.06178772926937198\n",
      "Time: 49.33208870887756 secs\n",
      "Iteration 8 Completed, Maximum Change: 0.04804281972673041\n",
      "Time: 50.19971060752869 secs\n",
      "Iteration 9 Completed, Maximum Change: 0.03747704753889192\n",
      "Time: 44.96027231216431 secs\n",
      "Iteration 10 Completed, Maximum Change: 0.02943666385159116\n",
      "Time: 40.76466989517212 secs\n",
      "Iteration 11 Completed, Maximum Change: 0.023297538502868864\n",
      "Time: 41.72674298286438 secs\n",
      "Iteration 12 Completed, Maximum Change: 0.01901569153953983\n",
      "Time: 41.067235708236694 secs\n",
      "Iteration 13 Completed, Maximum Change: 0.01751791648418824\n",
      "Time: 40.8862578868866 secs\n",
      "Iteration 14 Completed, Maximum Change: 0.019278581956714752\n",
      "Time: 37.88565921783447 secs\n",
      "Iteration 15 Completed, Maximum Change: 0.019666827084358837\n",
      "Time: 32.599998235702515 secs\n",
      "Iteration 16 Completed, Maximum Change: 0.017856221533239458\n",
      "Time: 31.733744144439697 secs\n",
      "Iteration 17 Completed, Maximum Change: 0.014655478459708701\n",
      "Time: 31.74317216873169 secs\n",
      "Iteration 18 Completed, Maximum Change: 0.011158077913202252\n",
      "Time: 32.08421087265015 secs\n",
      "Iteration 19 Completed, Maximum Change: 0.00856625446767989\n",
      "Time: 31.82165217399597 secs\n",
      "Iteration 20 Completed, Maximum Change: 0.007476317468729621\n",
      "Time: 32.01265025138855 secs\n",
      "Iteration 21 Completed, Maximum Change: 0.006509602483983412\n",
      "Time: 31.9722683429718 secs\n",
      "Iteration 22 Completed, Maximum Change: 0.006104881741839152\n",
      "Time: 31.914496898651123 secs\n",
      "Iteration 23 Completed, Maximum Change: 0.006951678162272962\n",
      "Time: 31.63372492790222 secs\n",
      "Iteration 24 Completed, Maximum Change: 0.007256788399309575\n",
      "Time: 31.611143112182617 secs\n",
      "Iteration 25 Completed, Maximum Change: 0.006844952181708344\n",
      "Time: 32.17285370826721 secs\n",
      "Iteration 26 Completed, Maximum Change: 0.005934026660247012\n",
      "Time: 32.04076266288757 secs\n",
      "Iteration 27 Completed, Maximum Change: 0.005211125017879015\n",
      "Time: 31.784783124923706 secs\n",
      "Iteration 28 Completed, Maximum Change: 0.0050328780677143525\n",
      "Time: 32.15630888938904 secs\n",
      "Iteration 29 Completed, Maximum Change: -1\n",
      "Time: 32.03643727302551 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_iters = 0\n",
    "\n",
    "while n_iters < max_iters and has_converged == False:\n",
    "    start = time.time()\n",
    "    has_converged = True\n",
    "    max_change = -1\n",
    "\n",
    "    n_iters += 1\n",
    "    count = {}\n",
    "    total = {}\n",
    "    for index in range(train_size):\n",
    "        s_total = {}\n",
    "        for inp_word in inp[index]:\n",
    "            s_total[inp_word] = 0\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in t:\n",
    "                    t[(inp_word, tar_word)] = uniform\n",
    "                s_total[inp_word] += t[(inp_word, tar_word)]\n",
    "\n",
    "        for inp_word in inp[index]:\n",
    "            for tar_word in tar[index]:\n",
    "                if (inp_word, tar_word) not in count:\n",
    "                    count[(inp_word, tar_word)] = 0\n",
    "                count[(inp_word, tar_word)] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "                if tar_word not in total:\n",
    "                    total[tar_word] = 0\n",
    "                total[tar_word] += (t[(inp_word, tar_word)] / s_total[inp_word])\n",
    "\n",
    "    # estimating the probabilities\n",
    "\n",
    "    if fine_tune == 0:\n",
    "      updated = {}\n",
    "      # train for all valid word pairs s.t count(inp_word, tar_word) > 0\n",
    "      for index in range(train_size):\n",
    "          for tar_word in tar[index]:\n",
    "              for inp_word in inp[index]:\n",
    "                  if (inp_word, tar_word) in updated:\n",
    "                      continue\n",
    "                  updated[(inp_word, tar_word)] = 1\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.01:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "\n",
    "    elif fine_tune == 1:\n",
    "      # train it only for 1000 most frequent words in English and Cantonese\n",
    "      n_tar_words = 0\n",
    "      updates = 0\n",
    "\n",
    "      for tar_word_tuples in sorted(tar_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "          tar_word = tar_word_tuples[0]\n",
    "          n_tar_words += 1\n",
    "          if n_tar_words > len(tar_words)/word_factor_max:\n",
    "              break\n",
    "          n_inp_words = 0\n",
    "          for inp_word_tuples in sorted(inp_words.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "              inp_word = inp_word_tuples[0]\n",
    "              n_inp_words += 1\n",
    "              if n_inp_words > len(inp_words)/word_factor_max:\n",
    "                  break\n",
    "              if (inp_word, tar_word) not in count or tar_word not in total:\n",
    "                  continue\n",
    "                  # assume in this case: t[(inp_word, tar_word)] = uniform\n",
    "              else:\n",
    "                  if abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]) > 0.005:\n",
    "                      has_converged = False\n",
    "                      max_change = max(max_change, abs(t[(inp_word, tar_word)] - count[(inp_word, tar_word)] / total[tar_word]))\n",
    "                  t[(inp_word, tar_word)] = count[(inp_word, tar_word)] / total[tar_word]\n",
    "                \n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Change\", max_change, step=n_iters)\n",
    "\n",
    "    print(\"Iteration \" + str(n_iters) + \" Completed, Maximum Change: \" + str(max_change) + '\\nTime: {} secs'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6yaalpW64cA-",
    "outputId": "b0678767-12f0-42ed-ab2f-c4fd45fe9a45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('scratchmitedu', 'scratch'), 1.0)\n",
      "(('scratchmitedu', 'mit'), 1.0)\n",
      "(('scratchmitedu', 'edu'), 1.0)\n",
      "(('YourequietWhywontyouanswerme', '我同我傾偈玩嘅呢你粒聲唔出走咗去'), 1.0)\n",
      "(('KySportsRadiomarchmadness', 'marchmadness'), 1.0)\n",
      "(('KySportsRadiomarchmadness', 'KySportsRadio'), 1.0)\n",
      "(('HelenWalters', 'Walters'), 1.0)\n",
      "(('HelenWalters', 'Helen'), 1.0)\n",
      "(('我', 'I'), 0.9957770178153831)\n",
      "(('拜', 'Bye'), 0.9012037665017589)\n",
      "(('你', 'You'), 0.8645979901999555)\n",
      "(('佢', 'He'), 0.8623040253821986)\n",
      "(('北', 'North'), 0.8584066958740497)\n",
      "(('我', 'me'), 0.842962543937259)\n",
      "(('人', 'people'), 0.8409003443977158)\n",
      "(('你', 'you'), 0.8385149894330988)\n",
      "(('玩', 'play'), 0.8035993143646347)\n",
      "(('想', 'want'), 0.7893458530238417)\n",
      "(('班', '6'), 0.7824981847934045)\n",
      "(('唔', 't'), 0.7744452153923423)\n",
      "(('食', 'eat'), 0.7634971544072492)\n",
      "(('買', 'buying'), 0.7599678739707708)\n",
      "(('喺', 'begin'), 0.7539531178388976)\n",
      "(('JeffNorris', 'JN'), 0.7519514843772058)\n",
      "(('爾', 'Al'), 0.7474434452621433)\n",
      "(('鈴', 'bells'), 0.7452010102311233)\n",
      "(('耶', 'Yay'), 0.743906336929763)\n",
      "(('草', 'grass'), 0.7360113811132146)\n",
      "(('五', 'fifth'), 0.7284106578253442)\n",
      "(('年', 'years'), 0.7272470599962789)\n",
      "(('我', 'My'), 0.7239919999728275)\n",
      "(('數', 'counted'), 0.7220849200690054)\n",
      "(('佢', 'he'), 0.7188282782175596)\n",
      "(('前', 'ago'), 0.7184256715367108)\n",
      "(('聽', 'hear'), 0.718217080923748)\n",
      "(('唔', 'not'), 0.6924442701439304)\n",
      "(('佢', 'she'), 0.692414835160655)\n",
      "(('點', '00'), 0.6916736382011367)\n",
      "(('我', 'my'), 0.6866585497875869)\n",
      "(('牙', 'teeth'), 0.6756421675175135)\n"
     ]
    }
   ],
   "source": [
    "# displaying the most confident translation pairs\n",
    "limit = 40\n",
    "for element in sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True):\n",
    "  print(element)\n",
    "  limit -= 1\n",
    "  if limit <= 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DpMllrHFDYV-"
   },
   "outputs": [],
   "source": [
    "# saving the translation model\n",
    "file = open(\"translation_model.pkl\",\"wb\")\n",
    "pickle.dump(t, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4wQ0a-EARrIy"
   },
   "outputs": [],
   "source": [
    "# using the model trained until convergence\n",
    "# to use a saved model\n",
    "model_name = \"translation_model.pkl\"\n",
    "pickle_in = open(model_name,\"rb\")\n",
    "t = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Yaz1CgJoCllZ"
   },
   "outputs": [],
   "source": [
    "I = {}\n",
    "for index in range(train_size):\n",
    "    for inp_id in range(len(inp[index])):\n",
    "        length = len(inp[index])\n",
    "        if length not in I:\n",
    "            I[length] = {} # maps the positional difference to a tuple: (sum of t's, count)\n",
    "        for tar_id in range(len(tar[index])):\n",
    "            if (tar_id - inp_id) not in I[length]:\n",
    "                I[length][(tar_id - inp_id)] = [t[(inp[index][inp_id], tar[index][tar_id])], 1]\n",
    "            else:\n",
    "                I[length][(tar_id - inp_id)][0] += t[(inp[index][inp_id], tar[index][tar_id])]\n",
    "                I[length][(tar_id - inp_id)][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh6ogFdO2r_-",
    "outputId": "0d071c8f-b63d-407b-9d3a-c13e8308f2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "source": [
    "# viewing the available sentence lengths encountered during training\n",
    "sentence_lengths = []\n",
    "for key in I.keys():\n",
    "    if key not in sentence_lengths:\n",
    "        sentence_lengths.append(key)\n",
    "sentence_lengths.sort()\n",
    "print(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1CNo6HGz6lqK"
   },
   "outputs": [],
   "source": [
    "# computing the alignment probabilities\n",
    "# p[I][tar_id - inp_id] = p(i | i', I)\n",
    "\n",
    "p = {}\n",
    "for key in I.keys():\n",
    "    p[key] = {}\n",
    "    sum_val = 0\n",
    "    for diff in I[key].keys():\n",
    "        p[key][diff] = I[key][diff][0] / I[key][diff][1]\n",
    "        sum_val += p[key][diff]\n",
    "    for diff in p[key].keys():\n",
    "        p[key][diff] /= sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWI7HCoI81Qa",
    "outputId": "543de11e-d4e9-4b17-d13f-262a28b78541"
   },
   "outputs": [],
   "source": [
    "for index in range(train_size):\n",
    "    length_inp = len(inp[index])\n",
    "    length_tar = len(tar[index])\n",
    "    if length_tar - length_inp > 10 and length_inp == 1:\n",
    "        print(\"Length of English Sentence:\", str(length_inp))\n",
    "        print(\"Length of Cantonese Sentence:\", str(length_tar))\n",
    "        \n",
    "# there exists an English sentence with one token s.t the Cantonese translation contains 19 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yqzyU-2g9_jk"
   },
   "outputs": [],
   "source": [
    "# computing initial transitions\n",
    "init = {}\n",
    "for length in p:\n",
    "    max_prob = -1\n",
    "    max_jump = 0\n",
    "    for key in p[length].keys():\n",
    "        if p[length][key] > max_prob:\n",
    "            max_prob = p[length][key]\n",
    "            max_jump = key\n",
    "    init[length] = max_jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlM2-MzmExbc",
    "outputId": "c3791846-4116-4905-88db-6d78866b792b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Bigrams: 23466\n",
      "Number of Unique Unigrams: 6219\n"
     ]
    }
   ],
   "source": [
    "# computing the transition probabilities for Cantonese\n",
    "bigrams = {}\n",
    "unigrams = {}\n",
    "\n",
    "# training on the train_set\n",
    "def model(dataset_size, dataset_name):\n",
    "    global bigrams\n",
    "    global unigrams\n",
    "    for index in range(dataset_size):\n",
    "        token_A = ''\n",
    "        for tar_token in tar[index]:\n",
    "            if tar_token not in unigrams:\n",
    "                unigrams[tar_token] = 1\n",
    "            else:\n",
    "                unigrams[tar_token] += 1\n",
    "            \n",
    "            token_B = tar_token\n",
    "            if (token_A, token_B) not in bigrams:\n",
    "                bigrams[(token_A, token_B)] = 1\n",
    "            else:\n",
    "                bigrams[(token_A, token_B)] += 1\n",
    "            token_A = token_B\n",
    "\n",
    "model(train_size, 'tar_train')\n",
    "\n",
    "bigram_count = len(bigrams)\n",
    "unigram_count = len(unigrams)\n",
    "print(\"Number of Unique Bigrams:\", bigram_count)\n",
    "print(\"Number of Unique Unigrams:\", unigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AF98ePawMnaE",
    "outputId": "5f2fd7f7-3702-4fb0-807a-ee332d5ab102"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "computed_sentences = []\n",
    "total_BLEU = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 7: 0}\n",
    "null_BLEU_count = 0\n",
    "\n",
    "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
    "\n",
    "def find_translation(inp_token):\n",
    "    for element in sorted_t:\n",
    "        if element[0][0].lower() == inp_token:\n",
    "            return element[0][1]\n",
    "    return \"\"\n",
    "\n",
    "def get_prob(seq):\n",
    "    # bigram language model with laplace smoothing and backoff\n",
    "    if len(seq) < 2 or len(seq) > 10:\n",
    "        return 1\n",
    "    score = 0\n",
    "    token_A = ''\n",
    "    for tar_token in seq:\n",
    "        token_B = tar_token\n",
    "        if (token_A, token_B) not in bigrams:\n",
    "            if token_B not in unigrams:\n",
    "                continue\n",
    "            else:\n",
    "                score += unigrams[token_B] / unigram_count\n",
    "        else:\n",
    "            base_token_count = 0\n",
    "            if token_A in unigrams:\n",
    "                base_token_count = unigrams[token_A]\n",
    "            score += (bigrams[(token_A, token_B)] + 1) / (base_token_count + unigram_count)\n",
    "        token_A = token_B\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50 / 1696 in 11.588090658187866 s\n",
      "Progress: 100 / 1696 in 7.161604881286621 s\n",
      "Progress: 150 / 1696 in 5.680079221725464 s\n",
      "Progress: 200 / 1696 in 9.051053524017334 s\n",
      "Progress: 250 / 1696 in 3.231999635696411 s\n",
      "Progress: 300 / 1696 in 8.481085300445557 s\n",
      "Progress: 350 / 1696 in 8.628620147705078 s\n",
      "Progress: 400 / 1696 in 9.214543581008911 s\n",
      "Progress: 450 / 1696 in 3.4550232887268066 s\n",
      "Progress: 500 / 1696 in 5.717482805252075 s\n",
      "Progress: 550 / 1696 in 9.373687028884888 s\n",
      "Progress: 600 / 1696 in 7.069001197814941 s\n",
      "Progress: 650 / 1696 in 13.656596183776855 s\n",
      "Progress: 700 / 1696 in 4.925023317337036 s\n",
      "Progress: 750 / 1696 in 5.196526527404785 s\n",
      "Progress: 800 / 1696 in 6.1280272006988525 s\n",
      "Progress: 850 / 1696 in 8.086551427841187 s\n",
      "Progress: 900 / 1696 in 2.3639976978302 s\n",
      "Progress: 950 / 1696 in 11.778563022613525 s\n",
      "Progress: 1000 / 1696 in 10.15955138206482 s\n",
      "Progress: 1050 / 1696 in 12.013608932495117 s\n",
      "Progress: 1100 / 1696 in 4.53650689125061 s\n",
      "Progress: 1150 / 1696 in 12.745169401168823 s\n",
      "Progress: 1200 / 1696 in 10.300063848495483 s\n",
      "Progress: 1250 / 1696 in 7.811018943786621 s\n",
      "Progress: 1300 / 1696 in 11.156041622161865 s\n",
      "Progress: 1350 / 1696 in 8.132047176361084 s\n",
      "Progress: 1400 / 1696 in 12.249104022979736 s\n",
      "Progress: 1450 / 1696 in 6.0676069259643555 s\n",
      "Progress: 1500 / 1696 in 8.003532409667969 s\n",
      "Progress: 1550 / 1696 in 8.138564825057983 s\n",
      "Progress: 1600 / 1696 in 8.901550769805908 s\n",
      "Progress: 1650 / 1696 in 10.289117574691772 s\n",
      "\n",
      "ToTal: 1696 / 1696 in 274.1866464614868 s\n"
     ]
    }
   ],
   "source": [
    "TotalTime = time.time()\n",
    "start = time.time()\n",
    "\n",
    "ori = []\n",
    "ref = []\n",
    "can = []\n",
    "num_test = 0\n",
    "\n",
    "for index in range(test_size):\n",
    "\n",
    "    translated_words = []\n",
    "    for inp_token in inp_test[index]:\n",
    "        translation = find_translation(inp_token)\n",
    "        if translation != \"\":\n",
    "            translated_words.append(translation)\n",
    "    perm = permutations(translated_words)\n",
    "    best_seq = translated_words\n",
    "    best_prob = -1\n",
    "\n",
    "    if len(inp_test[index]) < 10 and len(inp_test[index]) > 2:\n",
    "        for seq in perm:            \n",
    "            prob = get_prob(seq)\n",
    "            if prob > best_prob:\n",
    "                best_prob = prob\n",
    "                best_seq = seq\n",
    "            \n",
    "    ori.append(inp_test[index])\n",
    "    ref.append(tar_test[index])\n",
    "    can.append(best_seq)\n",
    "    num_test += 1\n",
    "    \n",
    "    if num_test % 50 == 0:\n",
    "        print(f'Progress: {num_test} / {test_size} in {time.time()-start} s')\n",
    "        start = time.time()\n",
    "        \n",
    "print(f'\\nToTal: {num_test} / {test_size} in {time.time()-TotalTime} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['隨', '著', '社', '會', '經', '濟', '水', '平', '提', '高']\n",
      "['By', 'elevating', 'to', 'a', 'higher', 'socio', 'economic', 'level']\n",
      "['carrying', 'wear', 'social', 'will', 'Periods', 'economics', 'water', 'peace', 'bear', 'high']\n",
      "BLEU: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import random\n",
    "ran = random.randint(1, len(ref))\n",
    "print(ori[ran])\n",
    "print(ref[ran])\n",
    "print(can[ran])\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "print('BLEU:', sentence_bleu(ref[ran], can[ran], smoothing_function=smoothie)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcR-JpzNT8gp",
    "outputId": "332fdf47-06f4-4e1b-8772-ba6b0357491a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-c: 0.0640808617334237\n"
     ]
    }
   ],
   "source": [
    "# Sentence-based and average score\n",
    "score = 0\n",
    "# for i in range(len(ref)):\n",
    "#     r = ref[i]\n",
    "#     c = can[i]\n",
    "#     score += sentence_bleu(r, c, smoothing_function=smoothie)*100\n",
    "# print('BLEU-s:', score/len(ref))\n",
    "\n",
    "# Corpus based, summing all nominator and denominator before division\n",
    "# r = [[r.split()] for r in ref]\n",
    "# c = [c.split() for c in can]\n",
    "score = corpus_bleu(ref, can, smoothing_function=smoothie)*100\n",
    "print('BLEU-c:', score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "SMT_English_to_Hindi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
